{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kshitij04Poojary/Iterated-Prisoners-Dilemma/blob/main/IPDRPcode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-y_zNX8rsqge",
        "outputId": "5c80a849-a38b-4414-84de-6de49892daaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.30)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Installing collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.1 colorlog-6.8.2 optuna-3.6.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-06-09 11:13:25,066] A new study created in memory with name: no-name-59a044e4-824c-489f-93f6-5694440dbd28\n",
            "<ipython-input-1-b6ba358bb597>:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
            "<ipython-input-1-b6ba358bb597>:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  epsilon_decay = trial.suggest_uniform('epsilon_decay', 0.99, 0.999)\n",
            "[I 2024-06-09 11:14:44,501] Trial 0 finished with value: 298.41 and parameters: {'hidden_size': 32, 'learning_rate': 0.0004426893419700587, 'batch_size': 128, 'epsilon_decay': 0.9922162223423997, 'target_update': 7}. Best is trial 0 with value: 298.41.\n",
            "<ipython-input-1-b6ba358bb597>:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
            "<ipython-input-1-b6ba358bb597>:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  epsilon_decay = trial.suggest_uniform('epsilon_decay', 0.99, 0.999)\n",
            "[I 2024-06-09 11:15:58,311] Trial 1 finished with value: 282.05 and parameters: {'hidden_size': 64, 'learning_rate': 0.00781675509629425, 'batch_size': 32, 'epsilon_decay': 0.9913323461719815, 'target_update': 9}. Best is trial 0 with value: 298.41.\n",
            "[I 2024-06-09 11:17:08,422] Trial 2 finished with value: 106.68 and parameters: {'hidden_size': 32, 'learning_rate': 0.0011267299296477864, 'batch_size': 64, 'epsilon_decay': 0.995811622254915, 'target_update': 17}. Best is trial 0 with value: 298.41.\n",
            "[I 2024-06-09 11:18:36,076] Trial 3 finished with value: 295.675 and parameters: {'hidden_size': 64, 'learning_rate': 0.006027436161828591, 'batch_size': 128, 'epsilon_decay': 0.9938440699840954, 'target_update': 17}. Best is trial 0 with value: 298.41.\n",
            "[I 2024-06-09 11:19:52,348] Trial 4 finished with value: 299.005 and parameters: {'hidden_size': 32, 'learning_rate': 0.0007839510206447523, 'batch_size': 128, 'epsilon_decay': 0.995319970813552, 'target_update': 7}. Best is trial 4 with value: 299.005.\n",
            "[I 2024-06-09 11:21:15,447] Trial 5 finished with value: 107.575 and parameters: {'hidden_size': 64, 'learning_rate': 0.0001835485899334731, 'batch_size': 128, 'epsilon_decay': 0.9968618966207018, 'target_update': 15}. Best is trial 4 with value: 299.005.\n",
            "[I 2024-06-09 11:22:25,978] Trial 6 finished with value: 183.965 and parameters: {'hidden_size': 64, 'learning_rate': 0.0005559722251387468, 'batch_size': 32, 'epsilon_decay': 0.9913184795794686, 'target_update': 5}. Best is trial 4 with value: 299.005.\n",
            "[I 2024-06-09 11:23:39,636] Trial 7 finished with value: 299.005 and parameters: {'hidden_size': 32, 'learning_rate': 0.00012743853209479678, 'batch_size': 128, 'epsilon_decay': 0.9969213294166704, 'target_update': 13}. Best is trial 4 with value: 299.005.\n",
            "[I 2024-06-09 11:24:55,418] Trial 8 finished with value: 106.575 and parameters: {'hidden_size': 32, 'learning_rate': 0.00012084574789910284, 'batch_size': 128, 'epsilon_decay': 0.9954295589251216, 'target_update': 18}. Best is trial 4 with value: 299.005.\n",
            "[I 2024-06-09 11:26:12,191] Trial 9 finished with value: 106.79 and parameters: {'hidden_size': 64, 'learning_rate': 0.0001421675783156248, 'batch_size': 64, 'epsilon_decay': 0.9919901325516035, 'target_update': 8}. Best is trial 4 with value: 299.005.\n",
            "[I 2024-06-09 11:27:41,866] Trial 10 finished with value: 290.07 and parameters: {'hidden_size': 128, 'learning_rate': 0.0024020161422626898, 'batch_size': 32, 'epsilon_decay': 0.9989938273915864, 'target_update': 11}. Best is trial 4 with value: 299.005.\n",
            "[I 2024-06-09 11:28:58,006] Trial 11 finished with value: 106.405 and parameters: {'hidden_size': 32, 'learning_rate': 0.00033643050291839484, 'batch_size': 128, 'epsilon_decay': 0.9974229789837751, 'target_update': 12}. Best is trial 4 with value: 299.005.\n",
            "[I 2024-06-09 11:30:13,705] Trial 12 finished with value: 191.265 and parameters: {'hidden_size': 32, 'learning_rate': 0.0014960305973444011, 'batch_size': 128, 'epsilon_decay': 0.9937304637224436, 'target_update': 14}. Best is trial 4 with value: 299.005.\n",
            "[I 2024-06-09 11:32:10,177] Trial 13 finished with value: 297.11 and parameters: {'hidden_size': 128, 'learning_rate': 0.003004565644438362, 'batch_size': 128, 'epsilon_decay': 0.9978782916701427, 'target_update': 20}. Best is trial 4 with value: 299.005.\n",
            "[I 2024-06-09 11:33:24,985] Trial 14 finished with value: 298.9 and parameters: {'hidden_size': 32, 'learning_rate': 0.0006855166511291883, 'batch_size': 128, 'epsilon_decay': 0.9950276205073327, 'target_update': 10}. Best is trial 4 with value: 299.005.\n",
            "[I 2024-06-09 11:34:32,891] Trial 15 finished with value: 299.08 and parameters: {'hidden_size': 32, 'learning_rate': 0.00022736624739326075, 'batch_size': 64, 'epsilon_decay': 0.996502370441819, 'target_update': 5}. Best is trial 15 with value: 299.08.\n",
            "[I 2024-06-09 11:36:11,710] Trial 16 finished with value: 188.97 and parameters: {'hidden_size': 128, 'learning_rate': 0.00026129208664329263, 'batch_size': 64, 'epsilon_decay': 0.993841905522996, 'target_update': 5}. Best is trial 15 with value: 299.08.\n",
            "[I 2024-06-09 11:37:21,312] Trial 17 finished with value: 298.865 and parameters: {'hidden_size': 32, 'learning_rate': 0.0002531417876480924, 'batch_size': 64, 'epsilon_decay': 0.9900089265130781, 'target_update': 7}. Best is trial 15 with value: 299.08.\n",
            "[I 2024-06-09 11:38:30,963] Trial 18 finished with value: 298.05 and parameters: {'hidden_size': 32, 'learning_rate': 0.0017532359360724812, 'batch_size': 64, 'epsilon_decay': 0.9961888403329308, 'target_update': 5}. Best is trial 15 with value: 299.08.\n",
            "[I 2024-06-09 11:39:38,593] Trial 19 finished with value: 106.315 and parameters: {'hidden_size': 32, 'learning_rate': 0.0007177733063833062, 'batch_size': 64, 'epsilon_decay': 0.9980857820257475, 'target_update': 8}. Best is trial 15 with value: 299.08.\n",
            "[I 2024-06-09 11:41:15,150] Trial 20 finished with value: 297.115 and parameters: {'hidden_size': 128, 'learning_rate': 0.004295860460574572, 'batch_size': 64, 'epsilon_decay': 0.9946416063269121, 'target_update': 7}. Best is trial 15 with value: 299.08.\n",
            "[I 2024-06-09 11:42:29,362] Trial 21 finished with value: 298.97 and parameters: {'hidden_size': 32, 'learning_rate': 0.0001059614006720212, 'batch_size': 128, 'epsilon_decay': 0.9964855249173107, 'target_update': 14}. Best is trial 15 with value: 299.08.\n",
            "[I 2024-06-09 11:43:43,903] Trial 22 finished with value: 107.1 and parameters: {'hidden_size': 32, 'learning_rate': 0.0001761996939521123, 'batch_size': 128, 'epsilon_decay': 0.9971395321016023, 'target_update': 12}. Best is trial 15 with value: 299.08.\n",
            "[I 2024-06-09 11:44:49,577] Trial 23 finished with value: 106.385 and parameters: {'hidden_size': 32, 'learning_rate': 0.00034504515354309717, 'batch_size': 32, 'epsilon_decay': 0.9987054829272723, 'target_update': 10}. Best is trial 15 with value: 299.08.\n",
            "[I 2024-06-09 11:46:05,714] Trial 24 finished with value: 298.765 and parameters: {'hidden_size': 32, 'learning_rate': 0.0002307154492905179, 'batch_size': 128, 'epsilon_decay': 0.9957347223918404, 'target_update': 6}. Best is trial 15 with value: 299.08.\n",
            "[I 2024-06-09 11:47:15,178] Trial 25 finished with value: 299.015 and parameters: {'hidden_size': 32, 'learning_rate': 0.0008449025927645858, 'batch_size': 64, 'epsilon_decay': 0.9965706412785622, 'target_update': 15}. Best is trial 15 with value: 299.08.\n",
            "[I 2024-06-09 11:48:21,822] Trial 26 finished with value: 298.51 and parameters: {'hidden_size': 32, 'learning_rate': 0.0009126183444298524, 'batch_size': 64, 'epsilon_decay': 0.9943935910893743, 'target_update': 16}. Best is trial 15 with value: 299.08.\n",
            "[I 2024-06-09 11:49:30,223] Trial 27 finished with value: 182.395 and parameters: {'hidden_size': 32, 'learning_rate': 0.001563714823376031, 'batch_size': 64, 'epsilon_decay': 0.9963010732110077, 'target_update': 19}. Best is trial 15 with value: 299.08.\n",
            "[I 2024-06-09 11:50:40,177] Trial 28 finished with value: 106.485 and parameters: {'hidden_size': 32, 'learning_rate': 0.0009779366345639812, 'batch_size': 64, 'epsilon_decay': 0.9931133584324005, 'target_update': 6}. Best is trial 15 with value: 299.08.\n",
            "[I 2024-06-09 11:52:18,276] Trial 29 finished with value: 202.34 and parameters: {'hidden_size': 128, 'learning_rate': 0.0003916245680439293, 'batch_size': 64, 'epsilon_decay': 0.995228967981663, 'target_update': 9}. Best is trial 15 with value: 299.08.\n",
            "[I 2024-06-09 11:53:27,711] Trial 30 finished with value: 106.575 and parameters: {'hidden_size': 32, 'learning_rate': 0.0005079446649895215, 'batch_size': 64, 'epsilon_decay': 0.9976520567498025, 'target_update': 15}. Best is trial 15 with value: 299.08.\n",
            "[I 2024-06-09 11:54:43,013] Trial 31 finished with value: 106.2 and parameters: {'hidden_size': 32, 'learning_rate': 0.00018416160769511554, 'batch_size': 128, 'epsilon_decay': 0.99695422549594, 'target_update': 13}. Best is trial 15 with value: 299.08.\n",
            "[I 2024-06-09 11:55:48,561] Trial 32 finished with value: 106.425 and parameters: {'hidden_size': 32, 'learning_rate': 0.00014001484612202115, 'batch_size': 32, 'epsilon_decay': 0.998385330474586, 'target_update': 13}. Best is trial 15 with value: 299.08.\n",
            "[I 2024-06-09 11:57:06,289] Trial 33 finished with value: 118.39 and parameters: {'hidden_size': 32, 'learning_rate': 0.0006131982455252876, 'batch_size': 128, 'epsilon_decay': 0.9959129682992559, 'target_update': 11}. Best is trial 15 with value: 299.08.\n",
            "[I 2024-06-09 11:58:14,496] Trial 34 finished with value: 299.085 and parameters: {'hidden_size': 32, 'learning_rate': 0.00010015743203805684, 'batch_size': 64, 'epsilon_decay': 0.9966272901999059, 'target_update': 16}. Best is trial 34 with value: 299.085.\n",
            "[I 2024-06-09 11:59:29,538] Trial 35 finished with value: 257.11 and parameters: {'hidden_size': 64, 'learning_rate': 0.0012735662135698487, 'batch_size': 64, 'epsilon_decay': 0.9965397199909777, 'target_update': 17}. Best is trial 34 with value: 299.085.\n",
            "[I 2024-06-09 12:00:39,196] Trial 36 finished with value: 298.895 and parameters: {'hidden_size': 32, 'learning_rate': 0.000815887163733661, 'batch_size': 64, 'epsilon_decay': 0.9946956068226863, 'target_update': 16}. Best is trial 34 with value: 299.085.\n",
            "[I 2024-06-09 12:01:55,521] Trial 37 finished with value: 296.49 and parameters: {'hidden_size': 64, 'learning_rate': 0.008435324619385705, 'batch_size': 64, 'epsilon_decay': 0.9958327836067707, 'target_update': 15}. Best is trial 34 with value: 299.085.\n",
            "[I 2024-06-09 12:03:03,307] Trial 38 finished with value: 107.015 and parameters: {'hidden_size': 32, 'learning_rate': 0.0004065230202599881, 'batch_size': 64, 'epsilon_decay': 0.9974361543563385, 'target_update': 18}. Best is trial 34 with value: 299.085.\n",
            "[I 2024-06-09 12:04:10,116] Trial 39 finished with value: 242.645 and parameters: {'hidden_size': 32, 'learning_rate': 0.0023304667466881425, 'batch_size': 32, 'epsilon_decay': 0.9954826998045505, 'target_update': 6}. Best is trial 34 with value: 299.085.\n",
            "[I 2024-06-09 12:05:27,612] Trial 40 finished with value: 298.915 and parameters: {'hidden_size': 64, 'learning_rate': 0.000494457931081182, 'batch_size': 64, 'epsilon_decay': 0.9966018905462425, 'target_update': 16}. Best is trial 34 with value: 299.085.\n",
            "[I 2024-06-09 12:06:44,230] Trial 41 finished with value: 299.075 and parameters: {'hidden_size': 32, 'learning_rate': 0.00010333868919642315, 'batch_size': 128, 'epsilon_decay': 0.9970493233787123, 'target_update': 14}. Best is trial 34 with value: 299.085.\n",
            "[I 2024-06-09 12:07:59,595] Trial 42 finished with value: 106.49 and parameters: {'hidden_size': 32, 'learning_rate': 0.00016990126486619806, 'batch_size': 128, 'epsilon_decay': 0.9970833694307024, 'target_update': 14}. Best is trial 34 with value: 299.085.\n",
            "[I 2024-06-09 12:09:14,383] Trial 43 finished with value: 106.4 and parameters: {'hidden_size': 32, 'learning_rate': 0.00011419770177798946, 'batch_size': 128, 'epsilon_decay': 0.9961143612816006, 'target_update': 15}. Best is trial 34 with value: 299.085.\n",
            "[I 2024-06-09 12:10:23,134] Trial 44 finished with value: 106.37 and parameters: {'hidden_size': 32, 'learning_rate': 0.000102049514732714, 'batch_size': 64, 'epsilon_decay': 0.9981370924921869, 'target_update': 17}. Best is trial 34 with value: 299.085.\n",
            "[I 2024-06-09 12:11:39,532] Trial 45 finished with value: 296.175 and parameters: {'hidden_size': 32, 'learning_rate': 0.00015018158410745307, 'batch_size': 128, 'epsilon_decay': 0.9950283760061193, 'target_update': 14}. Best is trial 34 with value: 299.085.\n",
            "[I 2024-06-09 12:12:46,805] Trial 46 finished with value: 299.045 and parameters: {'hidden_size': 32, 'learning_rate': 0.00021855303481159422, 'batch_size': 32, 'epsilon_decay': 0.9967245592467804, 'target_update': 8}. Best is trial 34 with value: 299.085.\n",
            "[I 2024-06-09 12:13:53,901] Trial 47 finished with value: 106.17 and parameters: {'hidden_size': 32, 'learning_rate': 0.00021247366292635527, 'batch_size': 32, 'epsilon_decay': 0.9974201834095952, 'target_update': 18}. Best is trial 34 with value: 299.085.\n",
            "[I 2024-06-09 12:15:25,363] Trial 48 finished with value: 182.91 and parameters: {'hidden_size': 128, 'learning_rate': 0.00028292932860765705, 'batch_size': 32, 'epsilon_decay': 0.9966538402875281, 'target_update': 8}. Best is trial 34 with value: 299.085.\n",
            "[I 2024-06-09 12:16:32,396] Trial 49 finished with value: 106.435 and parameters: {'hidden_size': 32, 'learning_rate': 0.00013027294505570512, 'batch_size': 32, 'epsilon_decay': 0.9971455013366229, 'target_update': 16}. Best is trial 34 with value: 299.085.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial: {'hidden_size': 32, 'learning_rate': 0.00010015743203805684, 'batch_size': 64, 'epsilon_decay': 0.9966272901999059, 'target_update': 16}\n",
            "Episode 1, Total Reward: 226\n",
            "Episode 2, Total Reward: 168\n",
            "Episode 3, Total Reward: 130\n",
            "Episode 4, Total Reward: 110\n",
            "Episode 5, Total Reward: 107\n",
            "Episode 6, Total Reward: 104\n",
            "Episode 7, Total Reward: 104\n",
            "Episode 8, Total Reward: 107\n",
            "Episode 9, Total Reward: 104\n",
            "Episode 10, Total Reward: 107\n",
            "Episode 11, Total Reward: 104\n",
            "Episode 12, Total Reward: 107\n",
            "Episode 13, Total Reward: 104\n",
            "Episode 14, Total Reward: 107\n",
            "Episode 15, Total Reward: 104\n",
            "Episode 16, Total Reward: 104\n",
            "Episode 17, Total Reward: 104\n",
            "Episode 18, Total Reward: 104\n",
            "Episode 19, Total Reward: 110\n",
            "Episode 20, Total Reward: 107\n",
            "Episode 21, Total Reward: 104\n",
            "Episode 22, Total Reward: 110\n",
            "Episode 23, Total Reward: 107\n",
            "Episode 24, Total Reward: 104\n",
            "Episode 25, Total Reward: 104\n",
            "Episode 26, Total Reward: 104\n",
            "Episode 27, Total Reward: 104\n",
            "Episode 28, Total Reward: 104\n",
            "Episode 29, Total Reward: 110\n",
            "Episode 30, Total Reward: 110\n",
            "Episode 31, Total Reward: 104\n",
            "Episode 32, Total Reward: 104\n",
            "Episode 33, Total Reward: 104\n",
            "Episode 34, Total Reward: 104\n",
            "Episode 35, Total Reward: 104\n",
            "Episode 36, Total Reward: 104\n",
            "Episode 37, Total Reward: 104\n",
            "Episode 38, Total Reward: 104\n",
            "Episode 39, Total Reward: 104\n",
            "Episode 40, Total Reward: 107\n",
            "Episode 41, Total Reward: 110\n",
            "Episode 42, Total Reward: 104\n",
            "Episode 43, Total Reward: 107\n",
            "Episode 44, Total Reward: 107\n",
            "Episode 45, Total Reward: 113\n",
            "Episode 46, Total Reward: 104\n",
            "Episode 47, Total Reward: 110\n",
            "Episode 48, Total Reward: 104\n",
            "Episode 49, Total Reward: 104\n",
            "Episode 50, Total Reward: 104\n",
            "Episode 51, Total Reward: 104\n",
            "Episode 52, Total Reward: 104\n",
            "Episode 53, Total Reward: 104\n",
            "Episode 54, Total Reward: 104\n",
            "Episode 55, Total Reward: 104\n",
            "Episode 56, Total Reward: 107\n",
            "Episode 57, Total Reward: 110\n",
            "Episode 58, Total Reward: 107\n",
            "Episode 59, Total Reward: 107\n",
            "Episode 60, Total Reward: 104\n",
            "Episode 61, Total Reward: 104\n",
            "Episode 62, Total Reward: 109\n",
            "Episode 63, Total Reward: 107\n",
            "Episode 64, Total Reward: 104\n",
            "Episode 65, Total Reward: 110\n",
            "Episode 66, Total Reward: 107\n",
            "Episode 67, Total Reward: 107\n",
            "Episode 68, Total Reward: 104\n",
            "Episode 69, Total Reward: 107\n",
            "Episode 70, Total Reward: 104\n",
            "Episode 71, Total Reward: 104\n",
            "Episode 72, Total Reward: 104\n",
            "Episode 73, Total Reward: 104\n",
            "Episode 74, Total Reward: 104\n",
            "Episode 75, Total Reward: 104\n",
            "Episode 76, Total Reward: 107\n",
            "Episode 77, Total Reward: 104\n",
            "Episode 78, Total Reward: 104\n",
            "Episode 79, Total Reward: 107\n",
            "Episode 80, Total Reward: 104\n",
            "Episode 81, Total Reward: 104\n",
            "Episode 82, Total Reward: 104\n",
            "Episode 83, Total Reward: 104\n",
            "Episode 84, Total Reward: 104\n",
            "Episode 85, Total Reward: 104\n",
            "Episode 86, Total Reward: 104\n",
            "Episode 87, Total Reward: 104\n",
            "Episode 88, Total Reward: 104\n",
            "Episode 89, Total Reward: 104\n",
            "Episode 90, Total Reward: 107\n",
            "Episode 91, Total Reward: 107\n",
            "Episode 92, Total Reward: 104\n",
            "Episode 93, Total Reward: 110\n",
            "Episode 94, Total Reward: 107\n",
            "Episode 95, Total Reward: 104\n",
            "Episode 96, Total Reward: 104\n",
            "Episode 97, Total Reward: 104\n",
            "Episode 98, Total Reward: 104\n",
            "Episode 99, Total Reward: 107\n",
            "Episode 100, Total Reward: 107\n",
            "Episode 101, Total Reward: 104\n",
            "Episode 102, Total Reward: 107\n",
            "Episode 103, Total Reward: 107\n",
            "Episode 104, Total Reward: 110\n",
            "Episode 105, Total Reward: 104\n",
            "Episode 106, Total Reward: 107\n",
            "Episode 107, Total Reward: 104\n",
            "Episode 108, Total Reward: 104\n",
            "Episode 109, Total Reward: 104\n",
            "Episode 110, Total Reward: 104\n",
            "Episode 111, Total Reward: 107\n",
            "Episode 112, Total Reward: 107\n",
            "Episode 113, Total Reward: 104\n",
            "Episode 114, Total Reward: 110\n",
            "Episode 115, Total Reward: 104\n",
            "Episode 116, Total Reward: 104\n",
            "Episode 117, Total Reward: 107\n",
            "Episode 118, Total Reward: 104\n",
            "Episode 119, Total Reward: 110\n",
            "Episode 120, Total Reward: 104\n",
            "Episode 121, Total Reward: 104\n",
            "Episode 122, Total Reward: 104\n",
            "Episode 123, Total Reward: 104\n",
            "Episode 124, Total Reward: 113\n",
            "Episode 125, Total Reward: 103\n",
            "Episode 126, Total Reward: 104\n",
            "Episode 127, Total Reward: 104\n",
            "Episode 128, Total Reward: 107\n",
            "Episode 129, Total Reward: 107\n",
            "Episode 130, Total Reward: 107\n",
            "Episode 131, Total Reward: 104\n",
            "Episode 132, Total Reward: 104\n",
            "Episode 133, Total Reward: 104\n",
            "Episode 134, Total Reward: 104\n",
            "Episode 135, Total Reward: 104\n",
            "Episode 136, Total Reward: 107\n",
            "Episode 137, Total Reward: 107\n",
            "Episode 138, Total Reward: 104\n",
            "Episode 139, Total Reward: 104\n",
            "Episode 140, Total Reward: 104\n",
            "Episode 141, Total Reward: 104\n",
            "Episode 142, Total Reward: 104\n",
            "Episode 143, Total Reward: 104\n",
            "Episode 144, Total Reward: 107\n",
            "Episode 145, Total Reward: 107\n",
            "Episode 146, Total Reward: 104\n",
            "Episode 147, Total Reward: 107\n",
            "Episode 148, Total Reward: 107\n",
            "Episode 149, Total Reward: 104\n",
            "Episode 150, Total Reward: 104\n",
            "Episode 151, Total Reward: 104\n",
            "Episode 152, Total Reward: 107\n",
            "Episode 153, Total Reward: 107\n",
            "Episode 154, Total Reward: 104\n",
            "Episode 155, Total Reward: 107\n",
            "Episode 156, Total Reward: 104\n",
            "Episode 157, Total Reward: 104\n",
            "Episode 158, Total Reward: 104\n",
            "Episode 159, Total Reward: 110\n",
            "Episode 160, Total Reward: 107\n",
            "Episode 161, Total Reward: 104\n",
            "Episode 162, Total Reward: 104\n",
            "Episode 163, Total Reward: 104\n",
            "Episode 164, Total Reward: 104\n",
            "Episode 165, Total Reward: 107\n",
            "Episode 166, Total Reward: 104\n",
            "Episode 167, Total Reward: 104\n",
            "Episode 168, Total Reward: 104\n",
            "Episode 169, Total Reward: 104\n",
            "Episode 170, Total Reward: 104\n",
            "Episode 171, Total Reward: 104\n",
            "Episode 172, Total Reward: 104\n",
            "Episode 173, Total Reward: 104\n",
            "Episode 174, Total Reward: 104\n",
            "Episode 175, Total Reward: 104\n",
            "Episode 176, Total Reward: 104\n",
            "Episode 177, Total Reward: 104\n",
            "Episode 178, Total Reward: 104\n",
            "Episode 179, Total Reward: 110\n",
            "Episode 180, Total Reward: 107\n",
            "Episode 181, Total Reward: 104\n",
            "Episode 182, Total Reward: 104\n",
            "Episode 183, Total Reward: 107\n",
            "Episode 184, Total Reward: 104\n",
            "Episode 185, Total Reward: 110\n",
            "Episode 186, Total Reward: 104\n",
            "Episode 187, Total Reward: 104\n",
            "Episode 188, Total Reward: 104\n",
            "Episode 189, Total Reward: 104\n",
            "Episode 190, Total Reward: 107\n",
            "Episode 191, Total Reward: 110\n",
            "Episode 192, Total Reward: 107\n",
            "Episode 193, Total Reward: 104\n",
            "Episode 194, Total Reward: 104\n",
            "Episode 195, Total Reward: 104\n",
            "Episode 196, Total Reward: 107\n",
            "Episode 197, Total Reward: 107\n",
            "Episode 198, Total Reward: 104\n",
            "Episode 199, Total Reward: 107\n",
            "Episode 200, Total Reward: 104\n",
            "Episode 1, Total Reward: 110\n",
            "Episode 2, Total Reward: 116\n",
            "Episode 3, Total Reward: 116\n",
            "Episode 4, Total Reward: 116\n",
            "Episode 5, Total Reward: 118\n",
            "Episode 6, Total Reward: 125\n",
            "Episode 7, Total Reward: 112\n",
            "Episode 8, Total Reward: 116\n",
            "Episode 9, Total Reward: 119\n",
            "Episode 10, Total Reward: 113\n",
            "Episode 11, Total Reward: 115\n",
            "Episode 12, Total Reward: 121\n",
            "Episode 13, Total Reward: 130\n",
            "Episode 14, Total Reward: 119\n",
            "Episode 15, Total Reward: 113\n",
            "Episode 16, Total Reward: 121\n",
            "Episode 17, Total Reward: 118\n",
            "Episode 18, Total Reward: 113\n",
            "Episode 19, Total Reward: 107\n",
            "Episode 20, Total Reward: 113\n",
            "Episode 21, Total Reward: 131\n",
            "Episode 22, Total Reward: 110\n",
            "Episode 23, Total Reward: 118\n",
            "Episode 24, Total Reward: 116\n",
            "Episode 25, Total Reward: 118\n",
            "Episode 26, Total Reward: 115\n",
            "Episode 27, Total Reward: 119\n",
            "Episode 28, Total Reward: 116\n",
            "Episode 29, Total Reward: 110\n",
            "Episode 30, Total Reward: 110\n",
            "Episode 31, Total Reward: 124\n",
            "Episode 32, Total Reward: 110\n",
            "Episode 33, Total Reward: 134\n",
            "Episode 34, Total Reward: 113\n",
            "Episode 35, Total Reward: 121\n",
            "Episode 36, Total Reward: 119\n",
            "Episode 37, Total Reward: 116\n",
            "Episode 38, Total Reward: 127\n",
            "Episode 39, Total Reward: 127\n",
            "Episode 40, Total Reward: 122\n",
            "Episode 41, Total Reward: 121\n",
            "Episode 42, Total Reward: 116\n",
            "Episode 43, Total Reward: 110\n",
            "Episode 44, Total Reward: 131\n",
            "Episode 45, Total Reward: 124\n",
            "Episode 46, Total Reward: 113\n",
            "Episode 47, Total Reward: 137\n",
            "Episode 48, Total Reward: 113\n",
            "Episode 49, Total Reward: 116\n",
            "Episode 50, Total Reward: 116\n",
            "Episode 51, Total Reward: 116\n",
            "Episode 52, Total Reward: 123\n",
            "Episode 53, Total Reward: 116\n",
            "Episode 54, Total Reward: 121\n",
            "Episode 55, Total Reward: 128\n",
            "Episode 56, Total Reward: 107\n",
            "Episode 57, Total Reward: 129\n",
            "Episode 58, Total Reward: 131\n",
            "Episode 59, Total Reward: 112\n",
            "Episode 60, Total Reward: 119\n",
            "Episode 61, Total Reward: 115\n",
            "Episode 62, Total Reward: 113\n",
            "Episode 63, Total Reward: 113\n",
            "Episode 64, Total Reward: 113\n",
            "Episode 65, Total Reward: 116\n",
            "Episode 66, Total Reward: 113\n",
            "Episode 67, Total Reward: 119\n",
            "Episode 68, Total Reward: 116\n",
            "Episode 69, Total Reward: 116\n",
            "Episode 70, Total Reward: 116\n",
            "Episode 71, Total Reward: 115\n",
            "Episode 72, Total Reward: 122\n",
            "Episode 73, Total Reward: 119\n",
            "Episode 74, Total Reward: 119\n",
            "Episode 75, Total Reward: 124\n",
            "Episode 76, Total Reward: 112\n",
            "Episode 77, Total Reward: 125\n",
            "Episode 78, Total Reward: 116\n",
            "Episode 79, Total Reward: 122\n",
            "Episode 80, Total Reward: 124\n",
            "Episode 81, Total Reward: 110\n",
            "Episode 82, Total Reward: 126\n",
            "Episode 83, Total Reward: 116\n",
            "Episode 84, Total Reward: 122\n",
            "Episode 85, Total Reward: 110\n",
            "Episode 86, Total Reward: 113\n",
            "Episode 87, Total Reward: 128\n",
            "Episode 88, Total Reward: 122\n",
            "Episode 89, Total Reward: 119\n",
            "Episode 90, Total Reward: 133\n",
            "Episode 91, Total Reward: 126\n",
            "Episode 92, Total Reward: 116\n",
            "Episode 93, Total Reward: 119\n",
            "Episode 94, Total Reward: 129\n",
            "Episode 95, Total Reward: 124\n",
            "Episode 96, Total Reward: 119\n",
            "Episode 97, Total Reward: 122\n",
            "Episode 98, Total Reward: 119\n",
            "Episode 99, Total Reward: 113\n",
            "Episode 100, Total Reward: 113\n",
            "Average reward against TFT strategy: 118.53\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import optuna\n",
        "\n",
        "# Global definitions for common parameters\n",
        "input_size = 5\n",
        "output_size = 2\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.01\n",
        "gamma = 0.99\n",
        "num_episodes = 200\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim, layer_num):\n",
        "        super().__init__()\n",
        "        self.lstmLayer = nn.LSTM(in_dim, hidden_dim, layer_num, batch_first=True)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fcLayer = nn.Linear(hidden_dim, out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstmLayer(x)\n",
        "        out = self.relu(out[:, -1, :])  # Take the output of the last time step\n",
        "        out = self.fcLayer(out)\n",
        "        return out\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, state, action, reward, next_state):\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(None)\n",
        "        self.buffer[self.position] = (state, action, reward, next_state)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return zip(*random.sample(self.buffer, batch_size))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "class IteratedPrisonersDilemma:\n",
        "    def __init__(self):\n",
        "        self.num_actions = 2\n",
        "        self.payoff_matrix = np.array([[3, 0], [5, 1], [1, 5], [0, 0]])\n",
        "\n",
        "    def step(self, action1, action2):\n",
        "        reward1 = self.payoff_matrix[action1][action2]\n",
        "        reward2 = self.payoff_matrix[action2][action1]\n",
        "        return reward1, reward2\n",
        "\n",
        "def select_action(state, epsilon, policy_net, output_size):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(output_size)\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "            q_values = policy_net(state_tensor)\n",
        "            return q_values.argmax().item()\n",
        "\n",
        "def update_q_values(replay_buffer, batch_size, policy_net, target_net, optimizer, gamma):\n",
        "    if len(replay_buffer) < batch_size:\n",
        "        return\n",
        "    states, actions, rewards, next_states = replay_buffer.sample(batch_size)\n",
        "    states = torch.tensor(states, dtype=torch.float32).view(batch_size, 1, -1)\n",
        "    actions = torch.tensor(actions, dtype=torch.long)\n",
        "    rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "    next_states = torch.tensor(next_states, dtype=torch.float32).view(batch_size, 1, -1)\n",
        "\n",
        "    q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "    next_q_values = target_net(next_states).max(1)[0].detach()\n",
        "    expected_q_values = rewards + gamma * next_q_values\n",
        "\n",
        "    loss = nn.functional.mse_loss(q_values, expected_q_values)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "def train_dqn(trial):\n",
        "    # Hyperparameters to optimize\n",
        "    hidden_size = trial.suggest_categorical('hidden_size', [32, 64, 128])\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
        "    epsilon_decay = trial.suggest_uniform('epsilon_decay', 0.99, 0.999)\n",
        "    target_update = trial.suggest_int('target_update', 5, 20)\n",
        "\n",
        "    policy_net = DQN(input_size, hidden_size, output_size, 1)\n",
        "    target_net = DQN(input_size, hidden_size, output_size, 1)\n",
        "    target_net.load_state_dict(policy_net.state_dict())\n",
        "    target_net.eval()\n",
        "\n",
        "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
        "    replay_buffer = ReplayBuffer(capacity=10000)\n",
        "    ipd_env = IteratedPrisonersDilemma()\n",
        "    total_rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = [0, 0, 0, 0, 0]\n",
        "        total_reward = 0\n",
        "        for t in range(100):\n",
        "            epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-episode / epsilon_decay)\n",
        "            action = select_action(state, epsilon, policy_net, output_size)\n",
        "            opponent_action = state[0]\n",
        "            reward, opponent_reward = ipd_env.step(action, opponent_action)\n",
        "            next_state = [action, opponent_action, reward, opponent_reward, t]\n",
        "            replay_buffer.push(state, action, reward, next_state)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            update_q_values(replay_buffer, batch_size, policy_net, target_net, optimizer, gamma)\n",
        "            if t % target_update == 0:\n",
        "                target_net.load_state_dict(policy_net.state_dict())\n",
        "        total_rewards.append(total_reward)\n",
        "\n",
        "    avg_reward = np.mean(total_rewards)\n",
        "    return avg_reward\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(train_dqn, n_trials=50)\n",
        "\n",
        "print(f\"Best trial: {study.best_trial.params}\")\n",
        "\n",
        "def play_ipd(policy_net, num_episodes, strategy, epsilon=0.1):\n",
        "    ipd_env = IteratedPrisonersDilemma()\n",
        "    total_rewards = []\n",
        "    for episode in range(num_episodes):\n",
        "        state = [0, 0, 0, 0, 0]\n",
        "        total_reward = 0\n",
        "        opponent_last_action = 0\n",
        "        grim_trigger_active = False\n",
        "        for t in range(100):\n",
        "            with torch.no_grad():\n",
        "                if np.random.rand() < epsilon:\n",
        "                    action = np.random.randint(output_size)\n",
        "                else:\n",
        "                    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "                    q_values = policy_net(state_tensor)\n",
        "                    action = q_values.argmax().item()\n",
        "\n",
        "            if strategy == \"random\":\n",
        "                opponent_action = np.random.randint(2)\n",
        "            elif strategy == \"tft\":\n",
        "                opponent_action = opponent_last_action\n",
        "            elif strategy == \"grim_trigger\":\n",
        "                if grim_trigger_active:\n",
        "                    opponent_action = 1\n",
        "                else:\n",
        "                    opponent_action = 0\n",
        "                    if action == 1:\n",
        "                        grim_trigger_active = True\n",
        "            elif strategy == \"always_cooperate\":\n",
        "                opponent_action = 0\n",
        "            elif strategy == \"always_defect\":\n",
        "                opponent_action = 1\n",
        "\n",
        "            reward, _ = ipd_env.step(action, opponent_action)\n",
        "            total_reward += reward\n",
        "\n",
        "            next_state = [action, opponent_action, reward, 0, t]\n",
        "            state = next_state\n",
        "            opponent_last_action = action\n",
        "\n",
        "        total_rewards.append(total_reward)\n",
        "        print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n",
        "\n",
        "    return total_rewards\n",
        "\n",
        "def test_against_tft(policy_net, num_episodes):\n",
        "    tft_rewards = play_ipd(policy_net, num_episodes, strategy=\"tft\", epsilon=0.1)\n",
        "    avg_reward = np.mean(tft_rewards)\n",
        "    print(\"Average reward against TFT strategy:\", avg_reward)\n",
        "\n",
        "# Use the best hyperparameters to train the final model\n",
        "best_params = study.best_trial.params\n",
        "hidden_size = best_params['hidden_size']\n",
        "learning_rate = best_params['learning_rate']\n",
        "batch_size = best_params['batch_size']\n",
        "epsilon_decay = best_params['epsilon_decay']\n",
        "target_update = best_params['target_update']\n",
        "\n",
        "# Train the final model\n",
        "policy_net = DQN(input_size, hidden_size, output_size, 1)\n",
        "target_net = DQN(input_size, hidden_size, output_size, 1)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
        "replay_buffer = ReplayBuffer(capacity=10000)\n",
        "ipd_env = IteratedPrisonersDilemma()\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = [0, 0, 0, 0, 0]\n",
        "    total_reward = 0\n",
        "    for t in range(100):\n",
        "        epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-episode / epsilon_decay)\n",
        "        action = select_action(state, epsilon, policy_net, output_size)\n",
        "        opponent_action = state[0]\n",
        "        reward, opponent_reward = ipd_env.step(action, opponent_action)\n",
        "        next_state = [action, opponent_action, reward, opponent_reward, t]\n",
        "        replay_buffer.push(state, action, reward, next_state)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        update_q_values(replay_buffer, batch_size, policy_net, target_net, optimizer, gamma)\n",
        "        if t % target_update == 0:\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "    print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n",
        "\n",
        "test_against_tft(policy_net, num_episodes=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Grim Trigger strategy\n",
        "def test_against_grim(policy_net, num_episodes):\n",
        "    grim_rewards = play_ipd(policy_net, num_episodes, strategy=\"grim_trigger\")\n",
        "    avg_reward = np.mean(grim_rewards)\n",
        "    print(f\"Average reward against Grim Trigger strategy: {avg_reward}\")\n",
        "\n",
        "test_against_grim(policy_net, num_episodes=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "86fgbz75Bi21",
        "outputId": "880273af-f61c-45f7-a786-5e5c97567fcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1, Total Reward: 100\n",
            "Episode 2, Total Reward: 100\n",
            "Episode 3, Total Reward: 99\n",
            "Episode 4, Total Reward: 99\n",
            "Episode 5, Total Reward: 98\n",
            "Episode 6, Total Reward: 96\n",
            "Episode 7, Total Reward: 102\n",
            "Episode 8, Total Reward: 95\n",
            "Episode 9, Total Reward: 99\n",
            "Episode 10, Total Reward: 98\n",
            "Episode 11, Total Reward: 101\n",
            "Episode 12, Total Reward: 97\n",
            "Episode 13, Total Reward: 100\n",
            "Episode 14, Total Reward: 97\n",
            "Episode 15, Total Reward: 99\n",
            "Episode 16, Total Reward: 95\n",
            "Episode 17, Total Reward: 104\n",
            "Episode 18, Total Reward: 97\n",
            "Episode 19, Total Reward: 99\n",
            "Episode 20, Total Reward: 98\n",
            "Episode 21, Total Reward: 100\n",
            "Episode 22, Total Reward: 101\n",
            "Episode 23, Total Reward: 96\n",
            "Episode 24, Total Reward: 99\n",
            "Episode 25, Total Reward: 99\n",
            "Episode 26, Total Reward: 102\n",
            "Episode 27, Total Reward: 100\n",
            "Episode 28, Total Reward: 102\n",
            "Episode 29, Total Reward: 97\n",
            "Episode 30, Total Reward: 100\n",
            "Episode 31, Total Reward: 98\n",
            "Episode 32, Total Reward: 94\n",
            "Episode 33, Total Reward: 100\n",
            "Episode 34, Total Reward: 98\n",
            "Episode 35, Total Reward: 96\n",
            "Episode 36, Total Reward: 98\n",
            "Episode 37, Total Reward: 96\n",
            "Episode 38, Total Reward: 102\n",
            "Episode 39, Total Reward: 98\n",
            "Episode 40, Total Reward: 99\n",
            "Episode 41, Total Reward: 100\n",
            "Episode 42, Total Reward: 103\n",
            "Episode 43, Total Reward: 101\n",
            "Episode 44, Total Reward: 101\n",
            "Episode 45, Total Reward: 102\n",
            "Episode 46, Total Reward: 100\n",
            "Episode 47, Total Reward: 98\n",
            "Episode 48, Total Reward: 102\n",
            "Episode 49, Total Reward: 96\n",
            "Episode 50, Total Reward: 100\n",
            "Episode 51, Total Reward: 98\n",
            "Episode 52, Total Reward: 96\n",
            "Episode 53, Total Reward: 101\n",
            "Episode 54, Total Reward: 101\n",
            "Episode 55, Total Reward: 97\n",
            "Episode 56, Total Reward: 97\n",
            "Episode 57, Total Reward: 98\n",
            "Episode 58, Total Reward: 99\n",
            "Episode 59, Total Reward: 100\n",
            "Episode 60, Total Reward: 98\n",
            "Episode 61, Total Reward: 97\n",
            "Episode 62, Total Reward: 98\n",
            "Episode 63, Total Reward: 101\n",
            "Episode 64, Total Reward: 100\n",
            "Episode 65, Total Reward: 100\n",
            "Episode 66, Total Reward: 100\n",
            "Episode 67, Total Reward: 98\n",
            "Episode 68, Total Reward: 99\n",
            "Episode 69, Total Reward: 96\n",
            "Episode 70, Total Reward: 98\n",
            "Episode 71, Total Reward: 99\n",
            "Episode 72, Total Reward: 99\n",
            "Episode 73, Total Reward: 102\n",
            "Episode 74, Total Reward: 100\n",
            "Episode 75, Total Reward: 103\n",
            "Episode 76, Total Reward: 99\n",
            "Episode 77, Total Reward: 93\n",
            "Episode 78, Total Reward: 102\n",
            "Episode 79, Total Reward: 98\n",
            "Episode 80, Total Reward: 99\n",
            "Episode 81, Total Reward: 98\n",
            "Episode 82, Total Reward: 100\n",
            "Episode 83, Total Reward: 102\n",
            "Episode 84, Total Reward: 95\n",
            "Episode 85, Total Reward: 103\n",
            "Episode 86, Total Reward: 99\n",
            "Episode 87, Total Reward: 97\n",
            "Episode 88, Total Reward: 94\n",
            "Episode 89, Total Reward: 99\n",
            "Episode 90, Total Reward: 97\n",
            "Episode 91, Total Reward: 100\n",
            "Episode 92, Total Reward: 101\n",
            "Episode 93, Total Reward: 98\n",
            "Episode 94, Total Reward: 102\n",
            "Episode 95, Total Reward: 98\n",
            "Episode 96, Total Reward: 97\n",
            "Episode 97, Total Reward: 99\n",
            "Episode 98, Total Reward: 100\n",
            "Episode 99, Total Reward: 99\n",
            "Episode 100, Total Reward: 100\n",
            "Average reward against Grim Trigger strategy: 98.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Always Cooperate strategy\n",
        "def test_against_ac(policy_net, num_episodes):\n",
        "    ac_rewards = play_ipd(policy_net, num_episodes, strategy=\"always_cooperate\")\n",
        "    avg_reward = np.mean(ac_rewards)\n",
        "    print(f\"Average reward against Always cooperate strategy: {avg_reward}\")\n",
        "\n",
        "test_against_ac(policy_net, num_episodes=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HDlOmvJEBvAg",
        "outputId": "3ed218c4-6dac-4eda-fb7d-942a0f93a207"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1, Total Reward: 488\n",
            "Episode 2, Total Reward: 480\n",
            "Episode 3, Total Reward: 490\n",
            "Episode 4, Total Reward: 492\n",
            "Episode 5, Total Reward: 496\n",
            "Episode 6, Total Reward: 480\n",
            "Episode 7, Total Reward: 486\n",
            "Episode 8, Total Reward: 492\n",
            "Episode 9, Total Reward: 494\n",
            "Episode 10, Total Reward: 486\n",
            "Episode 11, Total Reward: 496\n",
            "Episode 12, Total Reward: 492\n",
            "Episode 13, Total Reward: 494\n",
            "Episode 14, Total Reward: 492\n",
            "Episode 15, Total Reward: 492\n",
            "Episode 16, Total Reward: 492\n",
            "Episode 17, Total Reward: 494\n",
            "Episode 18, Total Reward: 494\n",
            "Episode 19, Total Reward: 490\n",
            "Episode 20, Total Reward: 492\n",
            "Episode 21, Total Reward: 492\n",
            "Episode 22, Total Reward: 492\n",
            "Episode 23, Total Reward: 494\n",
            "Episode 24, Total Reward: 490\n",
            "Episode 25, Total Reward: 496\n",
            "Episode 26, Total Reward: 486\n",
            "Episode 27, Total Reward: 494\n",
            "Episode 28, Total Reward: 488\n",
            "Episode 29, Total Reward: 484\n",
            "Episode 30, Total Reward: 496\n",
            "Episode 31, Total Reward: 484\n",
            "Episode 32, Total Reward: 486\n",
            "Episode 33, Total Reward: 488\n",
            "Episode 34, Total Reward: 490\n",
            "Episode 35, Total Reward: 490\n",
            "Episode 36, Total Reward: 492\n",
            "Episode 37, Total Reward: 492\n",
            "Episode 38, Total Reward: 490\n",
            "Episode 39, Total Reward: 496\n",
            "Episode 40, Total Reward: 494\n",
            "Episode 41, Total Reward: 482\n",
            "Episode 42, Total Reward: 484\n",
            "Episode 43, Total Reward: 490\n",
            "Episode 44, Total Reward: 484\n",
            "Episode 45, Total Reward: 484\n",
            "Episode 46, Total Reward: 490\n",
            "Episode 47, Total Reward: 486\n",
            "Episode 48, Total Reward: 494\n",
            "Episode 49, Total Reward: 490\n",
            "Episode 50, Total Reward: 490\n",
            "Episode 51, Total Reward: 494\n",
            "Episode 52, Total Reward: 492\n",
            "Episode 53, Total Reward: 494\n",
            "Episode 54, Total Reward: 494\n",
            "Episode 55, Total Reward: 494\n",
            "Episode 56, Total Reward: 488\n",
            "Episode 57, Total Reward: 494\n",
            "Episode 58, Total Reward: 486\n",
            "Episode 59, Total Reward: 494\n",
            "Episode 60, Total Reward: 490\n",
            "Episode 61, Total Reward: 496\n",
            "Episode 62, Total Reward: 494\n",
            "Episode 63, Total Reward: 492\n",
            "Episode 64, Total Reward: 492\n",
            "Episode 65, Total Reward: 492\n",
            "Episode 66, Total Reward: 488\n",
            "Episode 67, Total Reward: 488\n",
            "Episode 68, Total Reward: 486\n",
            "Episode 69, Total Reward: 490\n",
            "Episode 70, Total Reward: 490\n",
            "Episode 71, Total Reward: 484\n",
            "Episode 72, Total Reward: 494\n",
            "Episode 73, Total Reward: 488\n",
            "Episode 74, Total Reward: 478\n",
            "Episode 75, Total Reward: 488\n",
            "Episode 76, Total Reward: 494\n",
            "Episode 77, Total Reward: 494\n",
            "Episode 78, Total Reward: 494\n",
            "Episode 79, Total Reward: 492\n",
            "Episode 80, Total Reward: 494\n",
            "Episode 81, Total Reward: 492\n",
            "Episode 82, Total Reward: 494\n",
            "Episode 83, Total Reward: 486\n",
            "Episode 84, Total Reward: 492\n",
            "Episode 85, Total Reward: 498\n",
            "Episode 86, Total Reward: 494\n",
            "Episode 87, Total Reward: 492\n",
            "Episode 88, Total Reward: 490\n",
            "Episode 89, Total Reward: 494\n",
            "Episode 90, Total Reward: 486\n",
            "Episode 91, Total Reward: 488\n",
            "Episode 92, Total Reward: 496\n",
            "Episode 93, Total Reward: 486\n",
            "Episode 94, Total Reward: 484\n",
            "Episode 95, Total Reward: 496\n",
            "Episode 96, Total Reward: 490\n",
            "Episode 97, Total Reward: 488\n",
            "Episode 98, Total Reward: 484\n",
            "Episode 99, Total Reward: 494\n",
            "Episode 100, Total Reward: 488\n",
            "Average reward against Always cooperate strategy: 490.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Always Defect strategy\n",
        "def test_against_ad(policy_net, num_episodes):\n",
        "    ad_rewards = play_ipd(policy_net, num_episodes, strategy=\"always_defect\")\n",
        "    avg_reward = np.mean(ad_rewards)\n",
        "    print(f\"Average reward against Always defect strategy: {avg_reward}\")\n",
        "\n",
        "test_against_ad(policy_net, num_episodes=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wrwsJYAGByvG",
        "outputId": "fba2ecdc-f393-440d-f302-5db7fd736c7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1, Total Reward: 96\n",
            "Episode 2, Total Reward: 95\n",
            "Episode 3, Total Reward: 92\n",
            "Episode 4, Total Reward: 96\n",
            "Episode 5, Total Reward: 94\n",
            "Episode 6, Total Reward: 96\n",
            "Episode 7, Total Reward: 98\n",
            "Episode 8, Total Reward: 91\n",
            "Episode 9, Total Reward: 94\n",
            "Episode 10, Total Reward: 94\n",
            "Episode 11, Total Reward: 94\n",
            "Episode 12, Total Reward: 100\n",
            "Episode 13, Total Reward: 99\n",
            "Episode 14, Total Reward: 95\n",
            "Episode 15, Total Reward: 99\n",
            "Episode 16, Total Reward: 92\n",
            "Episode 17, Total Reward: 95\n",
            "Episode 18, Total Reward: 91\n",
            "Episode 19, Total Reward: 95\n",
            "Episode 20, Total Reward: 99\n",
            "Episode 21, Total Reward: 95\n",
            "Episode 22, Total Reward: 89\n",
            "Episode 23, Total Reward: 96\n",
            "Episode 24, Total Reward: 96\n",
            "Episode 25, Total Reward: 96\n",
            "Episode 26, Total Reward: 98\n",
            "Episode 27, Total Reward: 96\n",
            "Episode 28, Total Reward: 97\n",
            "Episode 29, Total Reward: 96\n",
            "Episode 30, Total Reward: 91\n",
            "Episode 31, Total Reward: 96\n",
            "Episode 32, Total Reward: 96\n",
            "Episode 33, Total Reward: 94\n",
            "Episode 34, Total Reward: 96\n",
            "Episode 35, Total Reward: 92\n",
            "Episode 36, Total Reward: 96\n",
            "Episode 37, Total Reward: 95\n",
            "Episode 38, Total Reward: 94\n",
            "Episode 39, Total Reward: 94\n",
            "Episode 40, Total Reward: 97\n",
            "Episode 41, Total Reward: 93\n",
            "Episode 42, Total Reward: 97\n",
            "Episode 43, Total Reward: 87\n",
            "Episode 44, Total Reward: 93\n",
            "Episode 45, Total Reward: 99\n",
            "Episode 46, Total Reward: 95\n",
            "Episode 47, Total Reward: 92\n",
            "Episode 48, Total Reward: 93\n",
            "Episode 49, Total Reward: 91\n",
            "Episode 50, Total Reward: 98\n",
            "Episode 51, Total Reward: 95\n",
            "Episode 52, Total Reward: 91\n",
            "Episode 53, Total Reward: 98\n",
            "Episode 54, Total Reward: 95\n",
            "Episode 55, Total Reward: 98\n",
            "Episode 56, Total Reward: 95\n",
            "Episode 57, Total Reward: 93\n",
            "Episode 58, Total Reward: 94\n",
            "Episode 59, Total Reward: 97\n",
            "Episode 60, Total Reward: 98\n",
            "Episode 61, Total Reward: 97\n",
            "Episode 62, Total Reward: 94\n",
            "Episode 63, Total Reward: 94\n",
            "Episode 64, Total Reward: 94\n",
            "Episode 65, Total Reward: 94\n",
            "Episode 66, Total Reward: 97\n",
            "Episode 67, Total Reward: 95\n",
            "Episode 68, Total Reward: 95\n",
            "Episode 69, Total Reward: 94\n",
            "Episode 70, Total Reward: 96\n",
            "Episode 71, Total Reward: 95\n",
            "Episode 72, Total Reward: 97\n",
            "Episode 73, Total Reward: 95\n",
            "Episode 74, Total Reward: 93\n",
            "Episode 75, Total Reward: 95\n",
            "Episode 76, Total Reward: 94\n",
            "Episode 77, Total Reward: 97\n",
            "Episode 78, Total Reward: 96\n",
            "Episode 79, Total Reward: 97\n",
            "Episode 80, Total Reward: 95\n",
            "Episode 81, Total Reward: 95\n",
            "Episode 82, Total Reward: 95\n",
            "Episode 83, Total Reward: 96\n",
            "Episode 84, Total Reward: 92\n",
            "Episode 85, Total Reward: 96\n",
            "Episode 86, Total Reward: 93\n",
            "Episode 87, Total Reward: 94\n",
            "Episode 88, Total Reward: 89\n",
            "Episode 89, Total Reward: 95\n",
            "Episode 90, Total Reward: 95\n",
            "Episode 91, Total Reward: 93\n",
            "Episode 92, Total Reward: 93\n",
            "Episode 93, Total Reward: 98\n",
            "Episode 94, Total Reward: 96\n",
            "Episode 95, Total Reward: 95\n",
            "Episode 96, Total Reward: 90\n",
            "Episode 97, Total Reward: 97\n",
            "Episode 98, Total Reward: 97\n",
            "Episode 99, Total Reward: 94\n",
            "Episode 100, Total Reward: 98\n",
            "Average reward against Always defect strategy: 94.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_against_random(policy_net, num_episodes):\n",
        "    random_rewards = play_ipd(policy_net, num_episodes,strategy='random')\n",
        "    avg_reward = np.mean(random_rewards)\n",
        "    print(\"Average reward against random strategy:\", avg_reward)\n",
        "\n",
        "\n",
        "test_against_random(policy_net, num_episodes=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "61emIBhJB2RZ",
        "outputId": "c9bc8638-0756-49dd-8c79-66648d708294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1, Total Reward: 323\n",
            "Episode 2, Total Reward: 283\n",
            "Episode 3, Total Reward: 286\n",
            "Episode 4, Total Reward: 341\n",
            "Episode 5, Total Reward: 305\n",
            "Episode 6, Total Reward: 291\n",
            "Episode 7, Total Reward: 318\n",
            "Episode 8, Total Reward: 294\n",
            "Episode 9, Total Reward: 238\n",
            "Episode 10, Total Reward: 298\n",
            "Episode 11, Total Reward: 303\n",
            "Episode 12, Total Reward: 273\n",
            "Episode 13, Total Reward: 286\n",
            "Episode 14, Total Reward: 300\n",
            "Episode 15, Total Reward: 324\n",
            "Episode 16, Total Reward: 295\n",
            "Episode 17, Total Reward: 307\n",
            "Episode 18, Total Reward: 308\n",
            "Episode 19, Total Reward: 314\n",
            "Episode 20, Total Reward: 330\n",
            "Episode 21, Total Reward: 292\n",
            "Episode 22, Total Reward: 306\n",
            "Episode 23, Total Reward: 306\n",
            "Episode 24, Total Reward: 286\n",
            "Episode 25, Total Reward: 308\n",
            "Episode 26, Total Reward: 314\n",
            "Episode 27, Total Reward: 324\n",
            "Episode 28, Total Reward: 295\n",
            "Episode 29, Total Reward: 290\n",
            "Episode 30, Total Reward: 288\n",
            "Episode 31, Total Reward: 297\n",
            "Episode 32, Total Reward: 275\n",
            "Episode 33, Total Reward: 291\n",
            "Episode 34, Total Reward: 278\n",
            "Episode 35, Total Reward: 290\n",
            "Episode 36, Total Reward: 287\n",
            "Episode 37, Total Reward: 306\n",
            "Episode 38, Total Reward: 267\n",
            "Episode 39, Total Reward: 288\n",
            "Episode 40, Total Reward: 274\n",
            "Episode 41, Total Reward: 293\n",
            "Episode 42, Total Reward: 308\n",
            "Episode 43, Total Reward: 289\n",
            "Episode 44, Total Reward: 258\n",
            "Episode 45, Total Reward: 285\n",
            "Episode 46, Total Reward: 317\n",
            "Episode 47, Total Reward: 295\n",
            "Episode 48, Total Reward: 285\n",
            "Episode 49, Total Reward: 305\n",
            "Episode 50, Total Reward: 299\n",
            "Episode 51, Total Reward: 295\n",
            "Episode 52, Total Reward: 305\n",
            "Episode 53, Total Reward: 314\n",
            "Episode 54, Total Reward: 288\n",
            "Episode 55, Total Reward: 283\n",
            "Episode 56, Total Reward: 302\n",
            "Episode 57, Total Reward: 261\n",
            "Episode 58, Total Reward: 275\n",
            "Episode 59, Total Reward: 299\n",
            "Episode 60, Total Reward: 316\n",
            "Episode 61, Total Reward: 295\n",
            "Episode 62, Total Reward: 315\n",
            "Episode 63, Total Reward: 297\n",
            "Episode 64, Total Reward: 272\n",
            "Episode 65, Total Reward: 318\n",
            "Episode 66, Total Reward: 280\n",
            "Episode 67, Total Reward: 286\n",
            "Episode 68, Total Reward: 301\n",
            "Episode 69, Total Reward: 274\n",
            "Episode 70, Total Reward: 318\n",
            "Episode 71, Total Reward: 299\n",
            "Episode 72, Total Reward: 281\n",
            "Episode 73, Total Reward: 259\n",
            "Episode 74, Total Reward: 268\n",
            "Episode 75, Total Reward: 277\n",
            "Episode 76, Total Reward: 321\n",
            "Episode 77, Total Reward: 329\n",
            "Episode 78, Total Reward: 280\n",
            "Episode 79, Total Reward: 278\n",
            "Episode 80, Total Reward: 271\n",
            "Episode 81, Total Reward: 309\n",
            "Episode 82, Total Reward: 293\n",
            "Episode 83, Total Reward: 278\n",
            "Episode 84, Total Reward: 293\n",
            "Episode 85, Total Reward: 316\n",
            "Episode 86, Total Reward: 322\n",
            "Episode 87, Total Reward: 256\n",
            "Episode 88, Total Reward: 292\n",
            "Episode 89, Total Reward: 299\n",
            "Episode 90, Total Reward: 301\n",
            "Episode 91, Total Reward: 248\n",
            "Episode 92, Total Reward: 270\n",
            "Episode 93, Total Reward: 267\n",
            "Episode 94, Total Reward: 300\n",
            "Episode 95, Total Reward: 293\n",
            "Episode 96, Total Reward: 284\n",
            "Episode 97, Total Reward: 294\n",
            "Episode 98, Total Reward: 280\n",
            "Episode 99, Total Reward: 297\n",
            "Episode 100, Total Reward: 273\n",
            "Average reward against random strategy: 293.35\n"
          ]
        }
      ]
    }
  ]
}