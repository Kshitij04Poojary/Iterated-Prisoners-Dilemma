{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kshitij04Poojary/Iterated-Prisoners-Dilemma/blob/main/DQN_IPD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim, layer_num):\n",
        "        super().__init__()\n",
        "        self.lstmLayer = nn.LSTM(in_dim, hidden_dim, layer_num)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fcLayer = nn.Linear(hidden_dim, out_dim)\n",
        "        self.weightInit = np.sqrt(1.0 / hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstmLayer(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fcLayer(out)\n",
        "        out = nn.Softmax(dim=-1)(out)\n",
        "        return out\n",
        "\n",
        "# Define the replay buffer\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, state, action, reward, next_state):\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(None)\n",
        "        self.buffer[self.position] = (state, action, reward, next_state)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return zip(*random.sample(self.buffer, batch_size))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# Define constants\n",
        "input_size = 5  # State representation size\n",
        "hidden_size = 64  # Hidden layer size\n",
        "output_size = 2  # Number of actions\n",
        "batch_size = 64\n",
        "gamma = 0.99  # Discount factor\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.01\n",
        "epsilon_decay = 0.995\n",
        "target_update = 10  # Update target network every 10 steps\n",
        "num_episodes = 200\n",
        "\n",
        "# Define the IPD environment\n",
        "class IteratedPrisonersDilemma:\n",
        "    def __init__(self):\n",
        "        self.num_actions = 2  # Cooperate or Defect\n",
        "        self.payoff_matrix = np.array([[3, 0], [5, 1], [1, 5], [0, 0]])  # Payoff matrix\n",
        "\n",
        "    def step(self, action1, action2):\n",
        "        reward1 = self.payoff_matrix[action1][action2]\n",
        "        reward2 = self.payoff_matrix[action2][action1]\n",
        "        return reward1, reward2\n",
        "\n",
        "# Initialize DQN, target DQN, optimizer\n",
        "policy_net = DQN(input_size, hidden_size, output_size,4)\n",
        "target_net = DQN(input_size, hidden_size, output_size,4)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=0.02)\n",
        "\n",
        "# Initialize replay buffer\n",
        "replay_buffer = ReplayBuffer(capacity=10000)\n",
        "\n",
        "# Epsilon-greedy action selection\n",
        "def select_action(state, epsilon):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(output_size)\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            # Add batch dimension to the state\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "            q_values = policy_net(state_tensor)\n",
        "            return q_values.argmax().item()\n",
        "\n",
        "# Update Q-values using DQN\n",
        "def update_q_values():\n",
        "    if len(replay_buffer) > batch_size:\n",
        "        states, actions, rewards, next_states = replay_buffer.sample(batch_size)\n",
        "        states = torch.tensor(states, dtype=torch.float32)\n",
        "        actions = torch.tensor(actions, dtype=torch.long)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
        "\n",
        "        q_values = policy_net(states)\n",
        "        q_values_next = target_net(next_states).max(1)[0].detach()\n",
        "        expected_q_values = rewards + gamma * q_values_next\n",
        "\n",
        "        loss = nn.functional.mse_loss(q_values.gather(1, actions.unsqueeze(1)), expected_q_values.unsqueeze(1))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Training loop\n",
        "ipd_env = IteratedPrisonersDilemma()\n",
        "for episode in range(num_episodes):\n",
        "    state = [0, 0, 0, 0, 0]  # Initial state\n",
        "    total_reward = 0\n",
        "    for t in range(100):  # Limiting episode length\n",
        "        # Select action\n",
        "        epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-episode / epsilon_decay)\n",
        "        action = select_action(state, epsilon)\n",
        "\n",
        "        # Take action\n",
        "        opponent_action = np.random.randint(2)  # Random opponent action\n",
        "        reward, opponent_reward = ipd_env.step(action, opponent_action)\n",
        "\n",
        "        # Store transition in replay buffer\n",
        "        next_state = [action, opponent_action, reward, opponent_reward, 0]  # Placeholder for next state\n",
        "        replay_buffer.push(state, action, reward, next_state)\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        # Perform one step of optimization (on minibatch)\n",
        "        update_q_values()\n",
        "\n",
        "        # Update target network\n",
        "        if t % target_update == 0:\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "    print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n",
        "\n",
        "# After training, you can use the policy_net to play the game\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "silU2uNlhV_j",
        "outputId": "b2683448-4b87-436d-c689-86caedea10d2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1, Total Reward: 199\n",
            "Episode 2, Total Reward: 282\n",
            "Episode 3, Total Reward: 266\n",
            "Episode 4, Total Reward: 299\n",
            "Episode 5, Total Reward: 264\n",
            "Episode 6, Total Reward: 304\n",
            "Episode 7, Total Reward: 290\n",
            "Episode 8, Total Reward: 275\n",
            "Episode 9, Total Reward: 312\n",
            "Episode 10, Total Reward: 268\n",
            "Episode 11, Total Reward: 340\n",
            "Episode 12, Total Reward: 326\n",
            "Episode 13, Total Reward: 300\n",
            "Episode 14, Total Reward: 300\n",
            "Episode 15, Total Reward: 304\n",
            "Episode 16, Total Reward: 276\n",
            "Episode 17, Total Reward: 284\n",
            "Episode 18, Total Reward: 292\n",
            "Episode 19, Total Reward: 277\n",
            "Episode 20, Total Reward: 314\n",
            "Episode 21, Total Reward: 311\n",
            "Episode 22, Total Reward: 304\n",
            "Episode 23, Total Reward: 299\n",
            "Episode 24, Total Reward: 322\n",
            "Episode 25, Total Reward: 308\n",
            "Episode 26, Total Reward: 300\n",
            "Episode 27, Total Reward: 360\n",
            "Episode 28, Total Reward: 267\n",
            "Episode 29, Total Reward: 284\n",
            "Episode 30, Total Reward: 264\n",
            "Episode 31, Total Reward: 267\n",
            "Episode 32, Total Reward: 264\n",
            "Episode 33, Total Reward: 293\n",
            "Episode 34, Total Reward: 320\n",
            "Episode 35, Total Reward: 308\n",
            "Episode 36, Total Reward: 311\n",
            "Episode 37, Total Reward: 286\n",
            "Episode 38, Total Reward: 272\n",
            "Episode 39, Total Reward: 310\n",
            "Episode 40, Total Reward: 302\n",
            "Episode 41, Total Reward: 296\n",
            "Episode 42, Total Reward: 272\n",
            "Episode 43, Total Reward: 327\n",
            "Episode 44, Total Reward: 268\n",
            "Episode 45, Total Reward: 316\n",
            "Episode 46, Total Reward: 312\n",
            "Episode 47, Total Reward: 307\n",
            "Episode 48, Total Reward: 304\n",
            "Episode 49, Total Reward: 324\n",
            "Episode 50, Total Reward: 280\n",
            "Episode 51, Total Reward: 283\n",
            "Episode 52, Total Reward: 300\n",
            "Episode 53, Total Reward: 284\n",
            "Episode 54, Total Reward: 304\n",
            "Episode 55, Total Reward: 308\n",
            "Episode 56, Total Reward: 275\n",
            "Episode 57, Total Reward: 304\n",
            "Episode 58, Total Reward: 304\n",
            "Episode 59, Total Reward: 316\n",
            "Episode 60, Total Reward: 292\n",
            "Episode 61, Total Reward: 276\n",
            "Episode 62, Total Reward: 291\n",
            "Episode 63, Total Reward: 303\n",
            "Episode 64, Total Reward: 316\n",
            "Episode 65, Total Reward: 266\n",
            "Episode 66, Total Reward: 280\n",
            "Episode 67, Total Reward: 314\n",
            "Episode 68, Total Reward: 295\n",
            "Episode 69, Total Reward: 284\n",
            "Episode 70, Total Reward: 302\n",
            "Episode 71, Total Reward: 291\n",
            "Episode 72, Total Reward: 308\n",
            "Episode 73, Total Reward: 304\n",
            "Episode 74, Total Reward: 312\n",
            "Episode 75, Total Reward: 304\n",
            "Episode 76, Total Reward: 264\n",
            "Episode 77, Total Reward: 280\n",
            "Episode 78, Total Reward: 324\n",
            "Episode 79, Total Reward: 310\n",
            "Episode 80, Total Reward: 278\n",
            "Episode 81, Total Reward: 267\n",
            "Episode 82, Total Reward: 285\n",
            "Episode 83, Total Reward: 332\n",
            "Episode 84, Total Reward: 302\n",
            "Episode 85, Total Reward: 304\n",
            "Episode 86, Total Reward: 368\n",
            "Episode 87, Total Reward: 312\n",
            "Episode 88, Total Reward: 292\n",
            "Episode 89, Total Reward: 328\n",
            "Episode 90, Total Reward: 290\n",
            "Episode 91, Total Reward: 272\n",
            "Episode 92, Total Reward: 310\n",
            "Episode 93, Total Reward: 263\n",
            "Episode 94, Total Reward: 312\n",
            "Episode 95, Total Reward: 304\n",
            "Episode 96, Total Reward: 308\n",
            "Episode 97, Total Reward: 324\n",
            "Episode 98, Total Reward: 302\n",
            "Episode 99, Total Reward: 318\n",
            "Episode 100, Total Reward: 320\n",
            "Episode 101, Total Reward: 312\n",
            "Episode 102, Total Reward: 300\n",
            "Episode 103, Total Reward: 316\n",
            "Episode 104, Total Reward: 320\n",
            "Episode 105, Total Reward: 300\n",
            "Episode 106, Total Reward: 313\n",
            "Episode 107, Total Reward: 332\n",
            "Episode 108, Total Reward: 296\n",
            "Episode 109, Total Reward: 260\n",
            "Episode 110, Total Reward: 286\n",
            "Episode 111, Total Reward: 316\n",
            "Episode 112, Total Reward: 290\n",
            "Episode 113, Total Reward: 274\n",
            "Episode 114, Total Reward: 296\n",
            "Episode 115, Total Reward: 292\n",
            "Episode 116, Total Reward: 291\n",
            "Episode 117, Total Reward: 272\n",
            "Episode 118, Total Reward: 272\n",
            "Episode 119, Total Reward: 280\n",
            "Episode 120, Total Reward: 316\n",
            "Episode 121, Total Reward: 308\n",
            "Episode 122, Total Reward: 304\n",
            "Episode 123, Total Reward: 332\n",
            "Episode 124, Total Reward: 284\n",
            "Episode 125, Total Reward: 300\n",
            "Episode 126, Total Reward: 300\n",
            "Episode 127, Total Reward: 282\n",
            "Episode 128, Total Reward: 308\n",
            "Episode 129, Total Reward: 300\n",
            "Episode 130, Total Reward: 312\n",
            "Episode 131, Total Reward: 340\n",
            "Episode 132, Total Reward: 283\n",
            "Episode 133, Total Reward: 271\n",
            "Episode 134, Total Reward: 294\n",
            "Episode 135, Total Reward: 300\n",
            "Episode 136, Total Reward: 296\n",
            "Episode 137, Total Reward: 330\n",
            "Episode 138, Total Reward: 320\n",
            "Episode 139, Total Reward: 310\n",
            "Episode 140, Total Reward: 299\n",
            "Episode 141, Total Reward: 339\n",
            "Episode 142, Total Reward: 316\n",
            "Episode 143, Total Reward: 298\n",
            "Episode 144, Total Reward: 300\n",
            "Episode 145, Total Reward: 312\n",
            "Episode 146, Total Reward: 316\n",
            "Episode 147, Total Reward: 316\n",
            "Episode 148, Total Reward: 304\n",
            "Episode 149, Total Reward: 300\n",
            "Episode 150, Total Reward: 276\n",
            "Episode 151, Total Reward: 276\n",
            "Episode 152, Total Reward: 312\n",
            "Episode 153, Total Reward: 275\n",
            "Episode 154, Total Reward: 271\n",
            "Episode 155, Total Reward: 284\n",
            "Episode 156, Total Reward: 309\n",
            "Episode 157, Total Reward: 328\n",
            "Episode 158, Total Reward: 308\n",
            "Episode 159, Total Reward: 304\n",
            "Episode 160, Total Reward: 272\n",
            "Episode 161, Total Reward: 292\n",
            "Episode 162, Total Reward: 302\n",
            "Episode 163, Total Reward: 306\n",
            "Episode 164, Total Reward: 282\n",
            "Episode 165, Total Reward: 288\n",
            "Episode 166, Total Reward: 290\n",
            "Episode 167, Total Reward: 315\n",
            "Episode 168, Total Reward: 284\n",
            "Episode 169, Total Reward: 291\n",
            "Episode 170, Total Reward: 304\n",
            "Episode 171, Total Reward: 295\n",
            "Episode 172, Total Reward: 306\n",
            "Episode 173, Total Reward: 258\n",
            "Episode 174, Total Reward: 300\n",
            "Episode 175, Total Reward: 328\n",
            "Episode 176, Total Reward: 304\n",
            "Episode 177, Total Reward: 290\n",
            "Episode 178, Total Reward: 300\n",
            "Episode 179, Total Reward: 300\n",
            "Episode 180, Total Reward: 264\n",
            "Episode 181, Total Reward: 314\n",
            "Episode 182, Total Reward: 262\n",
            "Episode 183, Total Reward: 348\n",
            "Episode 184, Total Reward: 316\n",
            "Episode 185, Total Reward: 312\n",
            "Episode 186, Total Reward: 290\n",
            "Episode 187, Total Reward: 302\n",
            "Episode 188, Total Reward: 304\n",
            "Episode 189, Total Reward: 314\n",
            "Episode 190, Total Reward: 304\n",
            "Episode 191, Total Reward: 288\n",
            "Episode 192, Total Reward: 282\n",
            "Episode 193, Total Reward: 320\n",
            "Episode 194, Total Reward: 324\n",
            "Episode 195, Total Reward: 316\n",
            "Episode 196, Total Reward: 304\n",
            "Episode 197, Total Reward: 296\n",
            "Episode 198, Total Reward: 284\n",
            "Episode 199, Total Reward: 292\n",
            "Episode 200, Total Reward: 323\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to play IPD using the policy network\n",
        "def play_ipd(policy_net, num_episodes):\n",
        "    ipd_env = IteratedPrisonersDilemma()\n",
        "    total_rewards = []\n",
        "    for episode in range(num_episodes):\n",
        "        state = [0, 0, 0, 0, 0]  # Initial state\n",
        "        total_reward = 0\n",
        "        for t in range(100):  # Limiting episode length\n",
        "            # Select action using the policy network\n",
        "            with torch.no_grad():\n",
        "\n",
        "                q_values = policy_net(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n",
        "                action = q_values.argmax().item()\n",
        "\n",
        "            # Random opponent action\n",
        "            opponent_action = np.random.randint(2)\n",
        "\n",
        "            # Get rewards from the environment\n",
        "            reward, _ = ipd_env.step(action, opponent_action)\n",
        "\n",
        "            # Update total reward\n",
        "            total_reward += reward\n",
        "\n",
        "            # Move to the next state (only for recording, not used)\n",
        "            next_state = [action, opponent_action, reward, 0, 0]\n",
        "\n",
        "            # Update state\n",
        "            state = next_state\n",
        "\n",
        "        total_rewards.append(total_reward)\n",
        "\n",
        "    return total_rewards\n",
        "\n",
        "# Test against random strategy\n",
        "def test_against_random(policy_net, num_episodes):\n",
        "    random_rewards = play_ipd(policy_net, num_episodes)\n",
        "    avg_reward = np.mean(random_rewards)\n",
        "    print(\"Average reward against random strategy:\", avg_reward)\n",
        "\n",
        "\n",
        "test_against_random(policy_net, num_episodes=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fY9UtspOht1a",
        "outputId": "704ba37a-28d4-49f8-bf36-3ab233e5b23d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average reward against random strategy: 298.36\n"
          ]
        }
      ]
    }
  ]
}