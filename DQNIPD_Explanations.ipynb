{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kshitij04Poojary/Iterated-Prisoners-Dilemma/blob/main/DQNIPD_Explanations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random"
      ],
      "metadata": {
        "id": "F1lOGSqObYS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following defines a simple feed-forward Neural Network with only 2 layers. We use pytorch to make it. This is the part we need to edit. Here you can integrate an rnn or an lstm instead of a 2 layer Neural Network."
      ],
      "metadata": {
        "id": "Cy9I-tPqaTa0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKh-XSJQZY8b"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size,output_size):\n",
        "    super(DQN,self).__init__()\n",
        "    self.fc1 = nn.Linear(input_size,hidden_size)\n",
        "    self.fc2 = nn.Linear(hidden_size,output_size)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = torch.relu(self.fc1(x))     #applies RELU( Rectified Linear Unit) activation function to the first layer to maintain the dimensionality(linearity) of the layer\n",
        "    x = self.fc2(x)                 #output doesnt require an activation function\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replay Buffer is basically a Circular Stack (Dequeue?). It is a data structure used to track and access transitions that the agent encounters. The 4 states are : state (current state), action (action taken by the agent according to the current state), reward (reward recieved immeadeatly by the agent for the action), next_state(state observed after the current action). The whole point of the replay buffer is that it allows the agent to learn from it's previous actions. In it's sample function, it picks up random transitions according to the batch size. This is then used to update the parameters of the agent's Neural Network. It is random to minimize the bias it can form due to recent experiences and allowing the agent to learn from diverse experiences."
      ],
      "metadata": {
        "id": "Pp7uiiaK3_KY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "  def __init__(self,cap):\n",
        "    self.buffer = []\n",
        "    self.cap = cap      #cap is the capacity of the buffer\n",
        "    self.pos = 0\n",
        "  def push(self,state,action,reward,next_state):  #push function to append a new transition in the buffer\n",
        "    if self.pos <= self.cap:\n",
        "      self.buffer.append(None)          #initialize the buffer with None\n",
        "    self.buffer[self.pos] = (state,action,reward,next_state)\n",
        "    self.pos = (self.pos + 1) % self.cap\n",
        "  def sample(self,batch):             #zips random batch of transitions according to the batch size and returns it\n",
        "    return zip(*random.sample(self.buffer,batch))\n",
        "  def len(self):\n",
        "    return len(self.buffer)\n"
      ],
      "metadata": {
        "id": "KQhEvnJ025Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters\n",
        "input_size = 5  # State representation size\n",
        "hidden_size = 64  # Hidden layer size\n",
        "output_size = 2  # Number of actions\n",
        "batch = 64\n",
        "gamma = 0.99  # Discount factor\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.01\n",
        "epsilon_decay = 0.995\n",
        "target_update = 10  # Update target network every 10 steps\n",
        "num_episodes = 200"
      ],
      "metadata": {
        "id": "z-KQ6Hpf7wlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The payoff matrix basically aims to maximize the profit, which in a non-iterated prisoner's dilemma is the one where 1 deflects and the other coorporates. The rewards are assigned with this in mind. Rows indicate the agent(player1) and columns the oppnent(player2).\n",
        "Both Coorporate : Agent - 3, Opp - 0\n",
        "\n",
        "1 Defect, 1 Coorporate : Agent - 5, Opp-1 (Agent Defects)\n",
        "\n",
        "1 Coorporate , 1 Defect : Agent - 1 , Opp - 5 (Opp Defects)\n",
        "\n",
        "Both Defect : Agent - 0, Opp - 0"
      ],
      "metadata": {
        "id": "WzSNfyc6_JJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IPD:\n",
        "  def __init__(self):\n",
        "    self.actions = 2 #Coorporate or Deflect\n",
        "    self.payoff = np.array([[3, 0], [5, 1], [1, 5], [0, 0]])\n",
        "\n",
        "  def reward_for_action(self,action1,action2):\n",
        "    reward1 = self.payoff[action1][action2]\n",
        "    reward2 = self.payoff[action2][action1]\n",
        "    return reward1,reward2"
      ],
      "metadata": {
        "id": "LEQ-HUVl72bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial steps of the DQN algorithm involves making a policy network and a target network that are one and the same. The target network is a replica of the policy network"
      ],
      "metadata": {
        "id": "e78Weh7gBzNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy_network = DQN(input_size,hidden_size,output_size)\n",
        "target_network = DQN(input_size,hidden_size,output_size)\n",
        "target_network.load_state_dict(policy_network.state_dict())\n",
        "target_network.eval()    #according to pytorch, it puts it in eval mode that avoids certain actions to take place or something\n",
        "optimizer = optim.Adam(policy_network.parameters(),lr=0.001)  #optimizer for the policy network params\n",
        "replay_buffer = ReplayBuffer(cap = 1000)"
      ],
      "metadata": {
        "id": "lSaQW_aV-Va5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below segment is a part of the DQN algorithm. Epsilon is the \"exploration\" parameter. In the below section, we check if the random number is less than epsilon, in which case we will choose exploration and pick a random action. Else, we choose to exploit the network's previous knowledge. This is done by calculating all q values of the policy network and picking the action with the highest Q Value."
      ],
      "metadata": {
        "id": "GULk4yeAjPNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_action(state,epsilon):\n",
        "  if np.random.rand() < epsilon:  #exploration\n",
        "    return np.random.randint(output_size) #pick random action\n",
        "  else:\n",
        "    with torch.no_grad():   #exploitation\n",
        "      q_values = policy_network(torch.tensor(state,dtype = torch.float32))  #calculates q values. converts state into a tensor. (torch no grad makes computation faster)\n",
        "      return q_values.argmax().item() #finds maximum q value and finds the scalar value of the index of the action which is a tensor and return it as action"
      ],
      "metadata": {
        "id": "2fwnVQUmCNUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below step is used to update the q values in the policy network itself (The target network is then later updated after some amt of time). We sample a mini batch from the replay buffer and then convert it to tensors. We then calculate the q_values(current state), next_q_values(next state) and expected q values using the Bellman Equation. We than calculate the loss with respect to the q values of actions actually taken by the agent and the expected q values. Loss is calculated by gradient back propogation and then the parameters that lead to the least amount of loss is updated in the policy network."
      ],
      "metadata": {
        "id": "6XpQYnib5KgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_q_values():\n",
        "  if len(replay_buffer) > batch:  #checks if there are enough transitions in the replay buffer to make a mini batch\n",
        "    state, action, reward, next_state = replay_buffer.sample(batch)\n",
        "    state = torch.tensor (state, dtype = torch.float32)  #convert state to tensor\n",
        "    action = torch.tensor (action, dtype = torch.long)   #actions are convert into long tensor because actions are integers and can either be 0 or 1\n",
        "    reward = torch.tensor (reward, dtype = torch.float32)\n",
        "    next_state = torch.tensor (next_state, dtype = torch.float32)\n",
        "\n",
        "    q_vals = policy_network(state)   #calculate q values of current state\n",
        "    next_q_vals = target_network(next_state).max(1)[0].detach()    #calculate q values of next state and finding its maximum\n",
        "    expected_q_val = reward + gamma* next_q_vals   #Bellman's Equation to find expected q values\n",
        "    loss = nn.functional.mse_loss (q_vals.gather(1,actions.unsqueeze(1)),expected_q_vals.unsqueeze(1))  # q_vals.gather(1,actions.unsqueeze(1)) gathers\n",
        "    # gathers q values corresponding to the particular action column\n",
        "    # actions.unsqueeze(1) creates a column tensor of actions and expected_q_vals.unsqueeze(1) creates a column tensor of expected q values\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()   #updates params of policy network that give the least amount of loss"
      ],
      "metadata": {
        "id": "ld7UTFIbHIke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the main training loop.\n",
        "\n",
        "epsilon = epsilon_end + (epsilon_start-epsilon_end)*np.exp(-episode/epsilon_decay).\n",
        "\n",
        "In the above formula. epsilon_end is the value of epsilon that is tending to zero. This is the target value of epsilon which represents the minimum exploration rate. epsilon_start is the value of epsilon that is closer to 1. It is the initial value of epsilon which represents the maximum exploration rate. epsilon_decay is the factor which is responsible for the decrease of epsilon over time, so that the Network can move from exploration of new values to exploiting its previous knowledge.\n",
        "\n",
        "np.exp(-episode/epsilon_decay).This is known as the exponential decay factor which is negative and progressively becomes more negative as episode number increases. This causes the epsilon value to tend to zero over time.\n",
        "(epsilon_start-epsilon_end)*np.exp(-episode/epsilon_decay).This essentially scales the value between epsilon start and epsilon end. Multiplying it with the exponential decay factor makes the epsilon value shift from epsilon start to epsilon end.\n",
        "epsilon_end + (epsilon_start-epsilon_end)*np.exp(-episode/epsilon_decay).Adding it to the epsilon_end makes it so that it allows the value of epsilon to decrease over time while also allowing exploration in the early stages of training."
      ],
      "metadata": {
        "id": "3QiZG6bgBlvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ipd = IPD()  #object of our IPD env\n",
        "for episode in range(num_episodes):\n",
        "  state = [0,0,0,0]  #initialization\n",
        "  total_reward = 0\n",
        "  for t in range(100):\n",
        "    epsilon = epsilon_end + (epsilon_start-epsilon_end)*np.exp(-episode/epsilon_decay)\n",
        "    action = select_action(state,epsilon)  #agent action selection\n",
        "    opp_action = np.random.randint(2)      #opponent action selection\n",
        "    reward, opp_reward = ipd.step(action,opp_action)\n",
        "    next_state = [action,opp_action,reward,opp_reward,0]\n",
        "    replay_buffer.push(state,action,reward,next_state)  #push transition states into replay buffer\n",
        "    state = next_state\n",
        "    total_reward = total_reward + reward\n",
        "    update_q_values()\n",
        "    if t % target_update == 0:\n",
        "      target_network.load_state_dict(policy_network.state_dict())  #load policy network params to the target network periodically. This is determined by the target_update param\n",
        "      #we defined and is called synchronization\n",
        "    print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")"
      ],
      "metadata": {
        "id": "vP0wSRFq9Jro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below stimulates and tests the code against Random Opponent Strategy for IPD."
      ],
      "metadata": {
        "id": "rRwUTHfaZug-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def play_IPD_random(policy_network,num_episodes):\n",
        "  ipd = IPD()\n",
        "  total_rewards = []\n",
        "  for episode in range(num_episodes):\n",
        "    state = [0,0,0,0]\n",
        "    total_reward=0\n",
        "    for t in range(100):\n",
        "      epsilon = epsilon_end + (epsilon_start-epsilon_end)*np.exp(-episode/epsilon_decay)\n",
        "      action = select_action(state,epsilon)\n",
        "      opp_action = np.random.randint(2)  #random opponent, this will change as we compare other stratergies\n",
        "      reward , _ = IPD.step(action,opp_action)  #we are only concerned with the agents rewards not the opponents\n",
        "      total_reward = total_reward+reward\n",
        "      next_state = [action,opp_action,reward,0,0]\n",
        "      state = next_state\n",
        "      total_rewards.append(total_reward)\n",
        "  return total_rewards\n",
        "\n",
        "\n",
        "def test_against_random(policy_network, num_episodes):\n",
        "  random_rewards = play_IPD_random(policy_network, num_episodes)\n",
        "  mean_reward = np.mean(random_rewards)  #finding mean reward for agent\n",
        "  print(\"Average reward against random strategy:\", mean_reward)\n",
        "\n",
        "test_against_random(policy_network, num_episodes=100)"
      ],
      "metadata": {
        "id": "4ktaZwc0PoeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following things need to be worked on after this :\n",
        "1.  input rnn/lstm\n",
        "2.  opponent stratergy (in training)\n",
        "3.  testing algos/stratergies (for eval)\n",
        "4.  hyperparameter optimization\n",
        "5. maybe integrate data?\n",
        "\n",
        "Resources : ChatGPT\n",
        "\n",
        "\n",
        "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
        "\n",
        "https://www.youtube.com/watch?v=t3fbETsIBCY\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YVGKhdRQaGk4"
      }
    }
  ]
}