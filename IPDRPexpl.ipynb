{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kshitij04Poojary/Iterated-Prisoners-Dilemma/blob/main/IPDRPexpl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import optuna"
      ],
      "metadata": {
        "id": "F1lOGSqObYS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Declaring Hyperparameters globally\n"
      ],
      "metadata": {
        "id": "xqlklz9R5pcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 5\n",
        "output_size = 2 #no of actions an agent can take\n",
        "epsilon_start = 1.0 #initial value of epsilon\n",
        "epsilon_end = 0.01 #final value of epsilon\n",
        "gamma = 0.99 #discount factor for rewards\n",
        "num_episodes = 200"
      ],
      "metadata": {
        "id": "15ARyIIn5tsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is a simple LSTM. This is what we use to create the DQN Model. Additionally, the output of this is the q values for each action (this needs to be raw q values, hence we dont have to use a softmax or any sort of activation function)\n",
        "\n"
      ],
      "metadata": {
        "id": "Cy9I-tPqaTa0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKh-XSJQZY8b"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim, layer_num):\n",
        "        super().__init__()\n",
        "        self.lstmLayer = nn.LSTM(in_dim, hidden_dim, layer_num, batch_first=True) #enabling batch_first ensures that the shape of tensors is maintained as [batch_size,sequence_length,layer_num]\n",
        "        self.relu = nn.ReLU() #applies RELU( Rectified Linear Unit) activation function introducing non linearity (model can learn more complex things)\n",
        "        self.fcLayer = nn.Linear(hidden_dim, out_dim) #fully connected layer to map from hidden to o/p state\n",
        "\n",
        "    def forward(self, x): #function to pass the data through the network layers where the input tensor is of shape [batch_size, seq_len, hidden_num]\n",
        "        out, _ = self.lstmLayer(x) #output features for each time step is stored\n",
        "        out = self.relu(out[:, -1, :])  # filtered to take o/p features of last time step only [shape of out changes from [batch_size, seq_len, layer_num] to [batch_size, hidden_num]]\n",
        "        out = self.fcLayer(out) # passes out to a fully connected layer making the dimension of output tensor [[batch_size, out_size]]\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replay Buffer is basically a Circular Stack (Dequeue?). It is a data structure used to track and access transitions that the agent encounters. The 4 states are : state (current state), action (action taken by the agent according to the current state), reward (reward recieved immeadeatly by the agent for the action), next_state(state observed after the current action). The whole point of the replay buffer is that it allows the agent to learn from it's previous actions. In it's sample function, it picks up random transitions according to the batch size. This is then used to update the parameters of the agent's Neural Network. It is random to minimize the bias it can form due to recent experiences and allowing the agent to learn from diverse experiences."
      ],
      "metadata": {
        "id": "Pp7uiiaK3_KY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "  def __init__(self,cap):\n",
        "    self.buffer = []\n",
        "    self.cap = cap      #cap is the capacity of the buffer\n",
        "    self.pos = 0\n",
        "  def push(self,state,action,reward,next_state):  #push function to append a new transition in the buffer\n",
        "    if len(self.buffer) < len(self.cap):\n",
        "      self.buffer.append(None)          #initialize the buffer with None\n",
        "    self.buffer[self.pos] = (state,action,reward,next_state)\n",
        "    self.pos = (self.pos + 1) % self.cap\n",
        "  def sample(self,batch):             #zips random batch of transitions according to the batch size and returns it\n",
        "    return zip(*random.sample(self.buffer,batch))\n",
        "  def len(self):\n",
        "    return len(self.buffer)\n"
      ],
      "metadata": {
        "id": "KQhEvnJ025Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The payoff matrix basically aims to maximize the profit, which in a non-iterated prisoner's dilemma is the one where 1 deflects and the other coorporates. The rewards are assigned with this in mind. Rows indicate the agent(player1) and columns the oppnent(player2).\n",
        "\n",
        "\n",
        "Both Coorporate : Agent - 3, Opp - 0\n",
        "\n",
        "1 Defect, 1 Coorporate : Agent - 5, Opp-1 (Agent Defects)\n",
        "\n",
        "1 Coorporate , 1 Defect : Agent - 1 , Opp - 5 (Opp Defects)\n",
        "\n",
        "Both Defect : Agent - 0, Opp - 0"
      ],
      "metadata": {
        "id": "WzSNfyc6_JJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IPD:\n",
        "  def __init__(self):\n",
        "    self.actions = 2 #Coorporate or Deflect\n",
        "    self.payoff = np.array([[3, 0], [5, 1], [1, 5], [0, 0]])\n",
        "\n",
        "  def step(self,action1,action2):\n",
        "    reward1 = self.payoff[action1][action2]\n",
        "    reward2 = self.payoff[action2][action1]\n",
        "    return reward1,reward2"
      ],
      "metadata": {
        "id": "LEQ-HUVl72bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below segment is a part of the DQN algorithm. Epsilon is the \"exploration\" parameter. In the below section, we check if the random number is less than epsilon, in which case we will choose exploration and pick a random action. Else, we choose to exploit the network's previous knowledge. This is done by calculating all q values of the policy network and picking the action with the highest Q Value.\n",
        "\n",
        "After unsqueeze operation 2 times the shape of the tensor becomes [1, 1, input_size]. because we process single step(seq_len) of a single batch(batch_size) at a time."
      ],
      "metadata": {
        "id": "GULk4yeAjPNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_action(state,epsilon):\n",
        "  if np.random.rand() < epsilon:  #exploration\n",
        "    return np.random.randint(output_size) #pick random action\n",
        "  else:\n",
        "    with torch.no_grad():   #exploitation\n",
        "      state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).unsqueeze(0) #calculates q values. converts state into a tensor. (torch no grad makes computation faster)\n",
        "      q_values = policy_net(state_tensor)\n",
        "      return q_values.argmax().item()#finds maximum q value and finds the scalar value of the index of the action which is a tensor and return it as action"
      ],
      "metadata": {
        "id": "2fwnVQUmCNUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below step is used to update the q values in the policy network itself (The target network is then later updated after some amt of time). We sample a mini batch from the replay buffer and then convert it to tensors. We then calculate the q_values(current state), next_q_values(next state) and expected q values using the Bellman Equation. We than calculate the loss with respect to the q values of actions actually taken by the agent and the expected q values. Loss is calculated by gradient back propogation and then the parameters that lead to the least amount of loss is updated in the policy network."
      ],
      "metadata": {
        "id": "6XpQYnib5KgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_q_values(replay_buffer, batch_size, policy_net, target_net, optimizer, gamma):\n",
        "  if len(replay_buffer) < batch_size:  #checks if there are enough transitions in the replay buffer to make a mini batch\n",
        "    return\n",
        "  state, action, reward, next_state = replay_buffer.sample(batch_size)\n",
        "  state = torch.tensor (state, dtype = torch.float32).view(batch_size, 1, -1)  #convert state to tensor and reshape it\n",
        "  action = torch.tensor (action, dtype = torch.long)   #actions are convert into long tensor because actions are integers and can either be 0 or 1\n",
        "  reward = torch.tensor (reward, dtype = torch.float32)\n",
        "  next_state = torch.tensor (next_state, dtype = torch.float32).view(batch_size, 1, -1) # use view to reshape it\n",
        "\n",
        "  q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)   #calculate q values of current state, adds a column tensor to reshape and then change it back to og dimension of(batch_size,)\n",
        "  next_q_values = target_network(next_state).max(1)[0].detach()    #calculate q values of next state and finding its maximum\n",
        "  expected_q_values = reward + gamma* next_q_vals   #Bellman's Equation to find expected q values\n",
        "  loss = nn.functional.mse_loss(q_values, expected_q_values)  #calculates loss b/w target q vals and predicted q vals\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()   #updates params of policy network that give the least amount of loss"
      ],
      "metadata": {
        "id": "ld7UTFIbHIke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the main training loop.\n",
        "Initial steps of the DQN algorithm involves making a policy network and a target network that are one and the same. The target network is a replica of the policy network\n",
        "\n",
        "epsilon = epsilon_end + (epsilon_start-epsilon_end)*np.exp(-episode/epsilon_decay).\n",
        "\n",
        "In the above formula. epsilon_end is the value of epsilon that is tending to zero. This is the target value of epsilon which represents the minimum exploration rate. epsilon_start is the value of epsilon that is closer to 1. It is the initial value of epsilon which represents the maximum exploration rate. epsilon_decay is the factor which is responsible for the decrease of epsilon over time, so that the Network can move from exploration of new values to exploiting its previous knowledge.\n",
        "\n",
        "np.exp(-episode/epsilon_decay).This is known as the exponential decay factor which is negative and progressively becomes more negative as episode number increases. This causes the epsilon value to tend to zero over time.\n",
        "(epsilon_start-epsilon_end)*np.exp(-episode/epsilon_decay).This essentially scales the value between epsilon start and epsilon end. Multiplying it with the exponential decay factor makes the epsilon value shift from epsilon start to epsilon end.\n",
        "epsilon_end + (epsilon_start-epsilon_end)*np.exp(-episode/epsilon_decay).Adding it to the epsilon_end makes it so that it allows the value of epsilon to decrease over time while also allowing exploration in the early stages of training."
      ],
      "metadata": {
        "id": "3QiZG6bgBlvP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overview of HyperParam Tuning:\n",
        "\n",
        "Optuna - Open Source FrameWork for hyperparam tuning. Uses Beysian Probab Approach to find the optimal params (probabilistic model of the obj func is built and then we minimize/maximise it)\n",
        "\n",
        "The study is an object that manages the optimization of the obj function. we have set it to maximise the obj function\n",
        "\n",
        "Suggestion Methods to define search range : suggest_categorical (pick one from list), uniform/loguniform/int(suggest a value uniformly/log value uniformly/integer value in the given range)"
      ],
      "metadata": {
        "id": "UZGqhPa8NPVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dqn(trial):\n",
        "    # Hyperparameters to optimize\n",
        "    hidden_size = trial.suggest_categorical('hidden_size', [32, 64, 128])\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
        "    epsilon_decay = trial.suggest_uniform('epsilon_decay', 0.99, 0.999)\n",
        "    target_update = trial.suggest_int('target_update', 5, 20)\n",
        "\n",
        "    policy_net = DQN(input_size, hidden_size, output_size, 1) #this network makes decisions\n",
        "    target_net = DQN(input_size, hidden_size, output_size, 1) #this network stabilizes the training processs\n",
        "    target_net.load_state_dict(policy_net.state_dict()) #copy wts from policy to target network\n",
        "    target_net.eval() #eval mode\n",
        "\n",
        "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate) # The learning rate is a hyperparameter that controls how much the optimizer adjusts the parameters with respect to the gradient. A higher learning rate means larger updates,faster learning process,\n",
        "    #but it might overshoot the optimal solution. Conversely, a lower learning rate means smaller updates, which could make the learning process slower but more precise.\n",
        "    replay_buffer = ReplayBuffer(capacity=10000)\n",
        "    ipd_env = IPD()\n",
        "    total_rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = [0, 0, 0, 0, 0] #initialization\n",
        "        total_reward = 0\n",
        "        for t in range(100):\n",
        "            epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-episode / epsilon_decay)\n",
        "            action = select_action(state, epsilon, policy_net, output_size) #selects agents current action\n",
        "            opponent_action = state[0] #opp ACTION TIT FOR TAT\n",
        "            reward, opponent_reward = ipd_env.step(action, opponent_action)\n",
        "            next_state = [action, opponent_action, reward, opponent_reward, t] # t is time step\n",
        "            replay_buffer.push(state, action, reward, next_state)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            update_q_values(replay_buffer, batch_size, policy_net, target_net, optimizer, gamma)\n",
        "            if t % target_update == 0:\n",
        "                target_net.load_state_dict(policy_net.state_dict())#load policy network params to the target network periodically. This is determined by the target_update param\n",
        "      #we defined and is called synchronization\n",
        "        total_rewards.append(total_reward)\n",
        "\n",
        "    avg_reward = np.mean(total_rewards)\n",
        "    return avg_reward\n",
        "\n",
        "study = optuna.create_study(direction='maximize') #creates an env that facilitates the hyperparameter tuning and aims to maximise the objective function\n",
        "study.optimize(train_dqn, n_trials=50) #objective function to optimize is train_dqn with 50 trails.\n",
        "\n",
        "print(f\"Best trial: {study.best_trial.params}\")"
      ],
      "metadata": {
        "id": "vP0wSRFq9Jro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the best/most rewarding hyperparams to train the final model!!"
      ],
      "metadata": {
        "id": "bD4tdNvkRZHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = study.best_trial.params #extract best hyperparams\n",
        "hidden_size = best_params['hidden_size']\n",
        "learning_rate = best_params['learning_rate']\n",
        "batch_size = best_params['batch_size']\n",
        "epsilon_decay = best_params['epsilon_decay']\n",
        "target_update = best_params['target_update']\n",
        "#rest is just a repeat of the training process\n",
        "policy_net = DQN(input_size, hidden_size, output_size, 1)\n",
        "target_net = DQN(input_size, hidden_size, output_size, 1)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
        "replay_buffer = ReplayBuffer(capacity=10000)\n",
        "ipd_env = IteratedPrisonersDilemma()\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = [0, 0, 0, 0, 0]\n",
        "    total_reward = 0\n",
        "    for t in range(100):\n",
        "        epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-episode / epsilon_decay)\n",
        "        action = select_action(state, epsilon, policy_net, output_size)\n",
        "        opponent_action = state[0]\n",
        "        reward, opponent_reward = ipd_env.step(action, opponent_action)\n",
        "        next_state = [action, opponent_action, reward, opponent_reward, t]\n",
        "        replay_buffer.push(state, action, reward, next_state)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        update_q_values(replay_buffer, batch_size, policy_net, target_net, optimizer, gamma)\n",
        "        if t % target_update == 0:\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "    print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")"
      ],
      "metadata": {
        "id": "fjj9eVcxRW8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simulate the IPD game. This function helps us simulate various stratergies to test out our network\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rRwUTHfaZug-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def play_ipd(policy_net, num_episodes, strategy, epsilon=0.1):\n",
        "    ipd_env = IPD()\n",
        "    total_rewards = []\n",
        "    for episode in range(num_episodes):\n",
        "        state = [0, 0, 0, 0, 0]\n",
        "        total_reward = 0\n",
        "        opponent_last_action = 0\n",
        "        grim_trigger_active = False\n",
        "        for t in range(100):\n",
        "            with torch.no_grad():\n",
        "                if np.random.rand() < epsilon: #action selection stratergy\n",
        "                    action = np.random.randint(output_size)\n",
        "                else:\n",
        "                    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "                    q_values = policy_net(state_tensor)\n",
        "                    action = q_values.argmax().item()\n",
        "\n",
        "            if strategy == \"random\":\n",
        "                opponent_action = np.random.randint(2)\n",
        "            elif strategy == \"tft\":\n",
        "                opponent_action = opponent_last_action\n",
        "            elif strategy == \"grim_trigger\":\n",
        "                if grim_trigger_active:\n",
        "                    opponent_action = 1\n",
        "                else:\n",
        "                    opponent_action = 0\n",
        "                    if action == 1:\n",
        "                        grim_trigger_active = True\n",
        "            elif strategy == \"always_cooperate\":\n",
        "                opponent_action = 0\n",
        "            elif strategy == \"always_defect\":\n",
        "                opponent_action = 1\n",
        "\n",
        "            reward, _ = ipd_env.step(action, opponent_action)\n",
        "            total_reward += reward\n",
        "\n",
        "            next_state = [action, opponent_action, reward, 0, t]\n",
        "            state = next_state\n",
        "            opponent_last_action = action\n",
        "\n",
        "        total_rewards.append(total_reward)\n",
        "        print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n",
        "\n",
        "    return total_rewards\n"
      ],
      "metadata": {
        "id": "4ktaZwc0PoeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TESTING AGAINST SOME COMMON STRATERGIES!!"
      ],
      "metadata": {
        "id": "X0-_Bwb0UFme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tit For Tat Stratergy\n",
        "def test_against_tft(policy_net, num_episodes):\n",
        "    tft_rewards = play_ipd(policy_net, num_episodes, strategy=\"tft\", epsilon=0.1)\n",
        "    avg_reward = np.mean(tft_rewards)\n",
        "    print(\"Average reward against TFT strategy:\", avg_reward)\n",
        "\n",
        "test_against_tft(policy_net, num_episodes=100)"
      ],
      "metadata": {
        "id": "u-45h3pZTm_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grim Trigger strategy\n",
        "def test_against_grim(policy_net, num_episodes):\n",
        "    grim_rewards = play_ipd(policy_net, num_episodes, strategy=\"grim_trigger\")\n",
        "    avg_reward = np.mean(grim_rewards)\n",
        "    print(f\"Average reward against Grim Trigger strategy: {avg_reward}\")\n",
        "\n",
        "test_against_grim(policy_net, num_episodes=100)"
      ],
      "metadata": {
        "id": "jRNrzFUUT45G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Always Cooperate strategy\n",
        "def test_against_ac(policy_net, num_episodes):\n",
        "    ac_rewards = play_ipd(policy_net, num_episodes, strategy=\"always_cooperate\")\n",
        "    avg_reward = np.mean(ac_rewards)\n",
        "    print(f\"Average reward against Always cooperate strategy: {avg_reward}\")\n",
        "\n",
        "test_against_ac(policy_net, num_episodes=100)"
      ],
      "metadata": {
        "id": "2lfbESdDT5gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Always Defect strategy\n",
        "def test_against_ad(policy_net, num_episodes):\n",
        "    ad_rewards = play_ipd(policy_net, num_episodes, strategy=\"always_defect\")\n",
        "    avg_reward = np.mean(ad_rewards)\n",
        "    print(f\"Average reward against Always defect strategy: {avg_reward}\")\n",
        "\n",
        "test_against_ad(policy_net, num_episodes=100)"
      ],
      "metadata": {
        "id": "JMPuSa6-T8D_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random stratergy\n",
        "def test_against_random(policy_net, num_episodes):\n",
        "    random_rewards = play_ipd(policy_net, num_episodes,strategy='random')\n",
        "    avg_reward = np.mean(random_rewards)\n",
        "    print(\"Average reward against random strategy:\", avg_reward)\n",
        "\n",
        "\n",
        "test_against_random(policy_net, num_episodes=100)"
      ],
      "metadata": {
        "id": "nYwGO2YNT95U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
        "\n",
        "https://www.youtube.com/watch?v=t3fbETsIBCY\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YVGKhdRQaGk4"
      }
    }
  ]
}